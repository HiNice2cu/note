# YOLO物体检测原理与应用

**核心定位**：端到端实时物体检测算法，将检测任务转化为回归问题，快速实现“找物体+画框”

## 一、核心思想

1. **核心目标**：从图像像素直接回归出**边界框坐标**和**类别概率**，端到端检测

2. **核心流程**（四步口诀：划网-预测-筛选-去重）

   - 输入图片 → 划分S×S网格 → 网格预测物体信息 → 非极大值抑制（NMS）筛选最终框

3. **关键规则**：物体由**中心点所在网格**负责预测

## 二、基本原理（以早期版本为例）

### 1. 输入输出格式

| 类型 | 内容         | 说明                                                         |
| ---- | ------------ | ------------------------------------------------------------ |
| 输入 | 图片+标签    | 标签包含：置信度`Pc`、框坐标`Bx/By/Bh/Bw`、类别概率（one-hot） |
| 输出 | 网格预测向量 | 例：8维向量`[Pc,Bx,By,Bh,Bw,C1,C2,C3]`                       |

### 2. 核心操作

- **网格划分**：S×S网格（常用19×19），每个网格预测固定长度向量

- **置信度** **`Pc`**：表示网格内有物体的概率（1=有，0=无）

- **坐标归一化**：`Bx/By`是相对于网格的位置（0-1），`Bh/Bw`是相对于整图的比例

### 3. 检测两步走

1. **置信度筛选**：过滤`Pc < 阈值`（如0.4）的预测框

2. **非极大值抑制（NMS）**

   - 目标：解决同一物体的重复框问题

   - 步骤：选置信度最高框 → 计算其他框与它的IoU → 删除`IoU>0.5`的框 → 重复至结束

## 三、YOLO版本演进（核心改进+适用场景）

| 版本 | 核心改进                                   | 适用场景                       |
| ---- | ------------------------------------------ | ------------------------------ |
| V1   | 单网格单框，端到端基础架构                 | 入门理解原理                   |
| V2   | 引入锚框（Anchor Boxes），单网格多框       | 检测不同形状物体               |
| V3   | 多尺度检测（浅层小物体，深层大物体）       | 大中小目标兼顾检测             |
| V4   | Mosaic增强、DropBlock、注意力机制          | 提升检测精度，复杂场景         |
| V7   | 统一3×3卷积核，简化网络                    | 追求推理速度，嵌入式部署       |
| V8   | 官方维护，支持检测/分割/姿态估计，代码友好 | 项目开发、研究基线（**首选**） |
| V9   | 优化梯度传播，增强浅层特征学习             | 深层网络精度提升研究           |

## 四、数据集准备

### 1. 数据集来源

- 现成平台：Roboflow（规范YOLO格式）、Kaggle

- 自制：实拍/爬虫 → LabelImg标注 → 半自动标注（少量标注→训练初级模型→伪标签修正）

### 2. 标准结构

```Plain Text
dataset/
├── images/ （训练/验证图片）
└── labels/ （对应标签文件）
```

### 3. 配置文件（data.yaml）

```YAML
train: 训练集路径  val: 验证集路径
nc: 类别数  names: [类别1, 类别2,...]
```