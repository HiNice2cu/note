{"/note/机器人项目笔记/ROS/ROS 基础流程.html":{"title":"ROS 基础流程","content":"# ROS 基础流程 # 一、学习前准备 1. **安装系统**：Ubuntu（可虚拟机安装） 2. **安装 ROS**：按教程安装对应版本 3. **安装 VSCode**：并安装 ROS 和 C++ 插件，否则无法补全代码 # 二、ROS 开发流程（八步） ## 第一步：创建工作空间 创建根文件夹（如 `my_ws`） 在其下创建 `src` 文件夹（存放代码） ## 第二步：创建功能包 进入 `src` 目录 执行命令： ```bash catkin_create_pkg 包名 roscpp ``` （依赖选 `roscpp`，其他后期再加） ## 第三步：编写简单代码 在功能包的 `src` 文件夹下创建 `.cpp` 文件 固定开头： ```cpp #include <ros/ros.h> int main(int argc, char **argv) { ros::init(argc, argv, \"节点名\"); ros::NodeHandle nh; ROS_INFO(\"Hello, ROS!\"); return 0; } ``` ## 第四步：设置编译规则 打开功能包中的 `CMakeLists.txt` 添加： ```cmake add_executable(可执行文件名 src/你的程序.cpp) target_link_libraries(可执行文件名 ${catkin_LIBRARIES}) ``` ## 第五步：编译 回到工作空间根目录 执行： ```bash catkin_make ``` 生成的可执行文件在`devel` 文件夹中 ## 第六步：生效环境变量 临时生效（当前终端）： ```bash source devel/setup.bash ``` ## 第七步：运行节点 1. 启动 ROS 核心（任意终端）： ```bash roscore ``` 1. 运行节点： ```bash rosrun 包名 可执行文件名 ``` ## 第八步：循环与话题通信 ### 循环结构（固定写法）： ```cpp ros::Rate loop_rate(10); // 10Hz while (ros::ok()) { // 你的代码 loop_rate.sleep(); } ``` ### 发布者（说话的人）： ```cpp ros::Publisher pub nh.advertise<消息类型>(\"话题名\", 10); // 发布消息： pub.publish(msg); ``` ### 订阅者（听话的人）： ```cpp void callback(const 消息类型::ConstPtr& msg) { // 处理消息 } ros::Subscriber sub nh.subscribe(\"话题名\", 10, callback); // 在循环后加： ros::spinOnce(); // 或 ros::spin(); ``` # 三、常用命令 `rostopic list`：查看所有话题 `rostopic echo /话题名`：查看话题消息 `rosnode list`：查看运行中的节点 `rosnode info /节点名`：查看节点详情 # 四、注意 新开终端要重新 `source devel/setup.bash` 先启动 `roscore` 再运行节点 C++ 需要编译，Python 不需要 功能包 代码模块，放在 `src` 下"},"/note/机器人项目笔记/ROS/ROS入门指南：什么是ROS？.html":{"title":"ROS入门指南：什么是ROS？","content":"# ROS入门指南：什么是ROS？ **核心定位**：机器人软件开发的标准化框架与工具集，类比“机器人界的安卓系统”，实现模块化、分布式开发，避免重复造轮子。 ## 一、ROS是什么？ ### 核心概念 **ROS** Robot Operating System（机器人操作系统），并非传统意义上的操作系统（如Windows、Linux），而是一套**机器人软件开发框架和工具集**。 核心价值：像安卓统一手机开发平台一样，为机器人提供标准化软件开发环境，实现功能模块复用与跨设备适配。 ## 二、ROS的诞生故事（类比安卓） 对比项安卓（手机）ROS（机器人） 诞生背景早期手机系统碎片化，开发困难，应用无法通用早期机器人软硬件定制化，每款机器人需重新开发软件 核心思想提供统一系统，实现应用跨品牌手机运行提供统一框架，实现功能模块跨机器人复用 关键人物Andy Rubin（被谷歌收购后壮大）Morgan Quigley（斯坦福大学与吴恩达共同提出） 开源生态谷歌Play商店，第三方应用生态繁荣ROS软件包（Package）生态，社区贡献大量功能模块 效果厂商省开发成本，开发者一次开发多平台运行开发者复用模块，快速构建复杂机器人系统 ## 三、ROS的核心设计思想（乐高积木式开发） 1. **模块化** 将复杂机器人软件拆分为独立小模块（称为**Node，节点**），每个节点负责单一功能（如传感器读取、运动控制、视觉处理）。 节点间通过标准接口通信，功能边界清晰。 2. **分布式** 节点可部署在不同计算机上，例如视觉处理节点运行在高性能电脑，运动控制节点运行在树莓派。 优点：实现负载均衡，提升系统性能，便于团队分工协作开发。 3. **松耦合** 节点独立开发、运行、调试，支持C++、Python等多语言编写。 新增或替换节点不影响其他模块，系统扩展性强。 ## 四、为什么需要ROS？ **避免重复造轮子**：可直接复用社区成熟功能模块（如建图、导航、机械臂控制），无需从零开发。 **标准化开发流程**：提供统一通信机制（Topic、Service、Action）与工具链（Rviz可视化、Gazebo仿真、rosbag数据记录）。 **促进协作与开源**：支持研究人员、企业、爱好者共享软件包，形成良性开源生态。 **快速原型开发**：像搭积木一样组合现有节点，快速实现功能，便于调试与迭代优化。 ## 五、常见问题解答 **Q1：ROS能控制我的单片机/电机/舵机吗？** 能。ROS通常运行在电脑或树莓派，通过串口、USB、蓝牙、Wi Fi与单片机通信；单片机负责底层硬件控制，ROS节点负责指令发送与传感器数据接收。 **Q2：一定要用Ubuntu才能运行ROS吗？** 首选Ubuntu，大部分ROS版本基于其开发，兼容性最佳；支持其他Linux发行版但社区支持有限，Windows和macOS有适配版本但成熟度不足。 **Q3：不会C++/Python能用ROS吗？** 可以。ROS节点主要用C++/Python编写，但可从使用现有节点入手，多数功能有现成包，无需自行编码。 **Q4：ROS适合做什么类型的机器人？** 几乎所有类型，包括移动机器人、机械臂、无人机、服务机器人、教育机器人等，只要需软件协调传感器与执行器，ROS均可提供支持。 ## 六、ROS典型应用场景 场景说明 SLAM建图与导航使用`gmapping`、`move_base`等包实现自主建图与路径规划 机械臂控制使用`MoveIt!`进行运动规划与控制 视觉处理结合`OpenCV`与`cv_bridge`实现图像识别、目标跟踪 仿真开发通过`Gazebo`仿真，无需真实硬件即可开发调试 数据记录与分析使用`rosbag`记录运行数据，便于回放分析与问题排查 ## 七、核心总结 ROS核心价值在于提供标准化、模块化的机器人开发工具链，让开发者像搭积木一样组合现有模块，站在开源社区的肩膀上快速实现功能，大幅降低开发成本，避免重复造轮子。"},"/note/机器人项目笔记/index.html":{"title":"","content":"# 机器人项目笔记"},"/note/机器人项目笔记/本地模型部署与强化学习算法/本地模型部署与强化学习算法简单概念.html":{"title":"本地模型部署与强化学习算法使用笔记","content":"# 本地模型部署与强化学习算法使用笔记 # 一、本地模型部署 指将训练好的人工智能模型（如大语言模型、图像识别模型等）安装在用户自己的设备或服务器上运行，而非依赖云端API服务。 ## 主要特点： 在本地运行：在电脑、手机、企业服务器上直接运行 数据隐私：数据无需上传到云端，安全性更高 低延迟：无需网络请求，响应速度快 降低成本：长期使用可能比云服务更经济 ## 常见场景： 智能客服系统、文档分析工具、图像处理软件等在企业内部部署。 # 二、强化学习算法使用 机器学习的一种方法，让AI智能体通过试错来学习最优策略。 ## 核心比喻： 就像训练宠物或玩游戏 智能体（AI）在环境中采取行动 获得奖励或惩罚作为反馈 目标是最大化累计奖励 ## 典型应用： 游戏AI（如AlphaGo） 机器人控制 资源管理 自动驾驶决策"},"/note/机器人项目笔记/本地模型部署与强化学习算法/index.html":{"title":"","content":"# 本地模型部署与强化学习算法"},"/note/机器人项目笔记/YOLO/1.2-YOLOv5 模型检测.html":{"title":"YOLOv5 模型检测","content":"# YOLOv5 模型检测 ## 一、YOLOv5 模型版本选型 ### 1. 模型家族版本差异 YOLOv5 提供 N、S、M、L、X 五个核心版本，结构框架一致，核心差异体现在参数量、推理速度与检测精度（MAP 指标），需根据硬件性能和业务需求选择： 模型版本参数量推理速度检测精度（MAP）适用场景 YOLOv5N1.9M（最小）最快最低轻量设备、实时性优先场景（如边缘设备） YOLOv5S中等（倒数第二小）较快中等平衡速度与精度，日常实操首选（前文环境验证默认模型） YOLOv5M/L递增中等中高PC 端、对精度有一定要求的场景 YOLOv5X最大最慢最高高性能 GPU 设备、精度优先场景（如精密检测） ### 2. 模型获取方式 自动下载：首次运行 `detect.py` 时，脚本会自动下载默认的 `yolov5s.pt` 模型（约 140M），保存至 YOLOv5 根目录。 手动下载：访问 YOLOv5 官方 GitHub 仓库 → 进入 Releases 页面 → 选择对应版本（如 V7.0）→ 下载所需模型文件（.pt 格式），手动放置到 YOLOv5 根目录（适合网络不佳时加速）。 ## 二、detect.py 核心参数详解 通过命令行指定参数调用 `detect.py`，核心语法：`python detect.py 参数名 参数值`，以下为高频必用参数及实操演示。 ### 1. weights（指定模型文件） 作用：指定用于推理的预训练模型或自定义训练模型，是检测的核心参数。 #### 实操示例 ```Bash # 使用 YOLOv5S 模型推理（默认模型，可省略 weights参数） python detect.py weights yolov5s.pt # 切换 YOLOv5X 模型推理（需提前下载 yolov5x.pt 至根目录） python detect.py weights yolov5x.pt ``` #### 效果对比 YOLOv5S：推理 2 张测试图，耗时分别为 120.19ms、3ms，检测结果为 4 人 + 1 辆车。 YOLOv5X：检测结果一致（精度略有提升），但推理时间显著增加，需权衡速度与精度选择。 ### 2. source（指定检测目标） 作用：定义检测对象的来源，支持多种输入类型，参数值根据目标类型灵活设置。 参数值类型示例功能说明 数字（摄像头）` source 0`调用电脑默认摄像头，进行实时视频检测 图片路径` source data/images/bus.jpg`检测单张图片（示例为 YOLOv5 自带测试图） 视频路径` source test_video.mp4`对本地视频文件逐帧检测 特殊值（屏幕）` source screen`对整个电脑屏幕实时检测，按 `Ctrl+C` 终止程序 #### 实操示例 ```Bash # 检测单张自带测试图 python detect.py weights yolov5s.pt source data/images/bus.jpg # 对屏幕实时检测 python detect.py weights yolov5s.pt source screen ``` 说明：检测结果默认保存至 `runs/detect/expX` 文件夹（X 为递增数字，区分多次检测结果）。 ### 3. conf thres（置信度阈值） 作用：过滤低置信度检测框，仅显示置信度高于该阈值的目标，取值范围 0~1，默认值 0.25。 阈值越低：检测框越多，可能包含误检目标（置信度低的候选框被保留）。 阈值越高：检测框越少，仅保留高置信度目标，可能过滤掉真实低置信度目标。 #### 实操示例 ```Bash # 调高阈值至 0.8（仅保留高置信度目标） python detect.py weights yolov5s.pt source data/images/bus.jpg conf thres 0.8 # 调低阈值至 0.05（保留更多候选框，易出现误检） python detect.py weights yolov5s.pt source data/images/bus.jpg conf thres 0.05 ``` #### 效果对比 阈值 0.8：仅检测到 1 个 person，领带等低置信度目标被过滤。 阈值 0.05：检测框数量激增，出现大量低置信度误检目标，干扰有效结果。 ### 4. iou thres（IOU 阈值） 作用：基于非极大抑制（NMS）算法过滤重叠检测框，与置信度阈值逻辑相反，取值范围 0~1。 阈值越低：过滤越严格，重叠框越少（仅保留最优框）。 阈值越高：保留的重叠框越多，可保留同一目标的多个候选框。 说明：无需深入理解 NMS 原理，实操中可根据重叠框情况微调，默认值 0.45 适用于多数场景。 ### 5. 其他常用参数 ` img size`：输入图像尺寸，默认 640（与 YOLOv5 预训练模型维度一致，建议不随意修改，避免影响精度）。 ` max det`：单张图片最大检测目标数量，默认值可满足日常需求，按需调整即可。 ` device`：指定检测设备，不手动设置则自动匹配（优先 GPU，无 GPU 则用 CPU）。 ` view img`：检测过程中弹窗显示结果图片，便于实时查看。 ` classes`：指定检测类别（如仅检测“人”），过滤无关类别，示例：` classes 0`（0 对应 COCO 数据集的 person 类别）。 ` no save`：不保存检测结果，仅实时查看，适合调试场景。 ## 三、基于 Torch Hub 的简化推理（轻量化封装） 前文 `detect.py` 代码量较大，封装或嵌入可视化界面时较繁琐，Torch Hub 提供极简方案，仅需几行代码即可完成检测，且支持加载本地模型。 ### 1. 环境准备（安装 Jupyter Lab） 在激活的 yolov5 虚拟环境中执行安装命令： ```Bash pip install jupyter lab ``` ### 2. 新建 Notebook 文件 1. 启动 Jupyter Lab：命令行输入 `jupyter lab`，自动打开浏览器界面。 2. 新建文件：点击「New」→ 选择「Notebook（Python 3）」，命名为 `hub_detect.ipynb`。 ### 3. 极简推理代码（支持本地模型） ```Python # 1. 从 Torch Hub 加载模型（本地模型需指定 source 'local'） import torch model torch.hub.load('./', 'custom', path 'yolov5s.pt', source 'local') # path 为本地模型路径 # 2. 指定检测目标（图片路径） img 'data/images/bus.jpg' # 可替换为自定义图片路径 # 3. 模型推理 results model(img) # 4. 展示检测结果 results.show() ``` #### 代码说明 加载本地模型：需将 `path` 参数设为本地 .pt 模型文件路径，同时指定 `source 'local'`。 结果输出：`results.show()` 弹窗显示检测结果，也可通过 `results.save()` 保存结果至本地。 优势：代码精简，易于封装和嵌入可视化界面，虽参数不如 `detect.py` 全面，但可满足多数检测需求。 ## 四、检测实战注意事项 1. 模型与环境匹配：确保使用的模型版本与 YOLOv5 源码版本兼容（本文均基于 V7.0，避免高版本模型适配低版本源码）。 2. 硬件性能适配：YOLOv5X 等大模型需高性能 GPU 支持，否则推理速度极慢，低配置设备优先选 YOLOv5N/S。 3. 参数微调逻辑：优先调整 ` conf thres` 优化检测精度，重叠框过多时微调` iou thres`。 4. 结果路径管理：多次检测后 `runs/detect/` 下会生成多个 exp 文件夹，建议按场景重命名，便于追溯结果。 5. 终止实时检测：调用摄像头或屏幕检测时，需在命令行按 `Ctrl+C` 终止程序，避免后台占用资源。"},"/note/机器人项目笔记/YOLO/3.2-YOLOv5 修改网络结构（以C2f为例）.html":{"title":"YOLOv5 修改网络结构（以C2f为例）","content":"# YOLOv5 修改网络结构（以C2f为例） ### 一、前置准备：借鉴YOLOv8代码 网络结构修改的核心是借鉴现有开源代码（YOLOv8），获取C2f模块相关实现，具体操作如下： #### 1. 代码借鉴来源 ```text # YOLOv8 开源项目地址 https://github.com/ultralytics/ultralytics ``` #### 2. 代码获取与分析步骤 1. 访问上述 GitHub 地址，下载 YOLOv8 项目的 ZIP 压缩包，获取完整代码库。 2. 对比 YOLOv5 与 YOLOv8 项目结构差异：YOLOv8 为支持命令行执行方式，做了额外设计，源码结构与 YOLOv5 有所不同。 3. 定位 YOLOv8 网络结构定义位置：进入 YOLOv8 代码的 `ultralytics/nn/modules` 目录，该目录对应 YOLOv5 中的 `models/commons.py`，存储所有网络模块定义。 4. 提取核心模块：从 `ultralytics/nn/modules` 中复制 C2f相关模块代码，用于后续 YOLOv5 网络修改。 ### 二、网络结构修改全流程（以C2f为例，4步完成） 核心修改顺序（从原文提取，无任何更改）：`models/commons.py` → 加入新增网络结构 → `models/yolo.py` → 设定网络结构的传参细节 → `models/yolov5*.yaml` → 修改现有模型结构配置文件 → `train.py` → 训练时指定模型结构配置文件 #### 第一步：修改 models/commons.py（加入C2f模块） 1. 打开 YOLOv5 项目的 `models/commons.py` 文件。 2. 将从 YOLOv8 `ultralytics/nn/modules` 中提取的 C2f（CRF）模块代码，复制到 `commons.py` 中（建议放在 C3 模块之前，便于后续替换）。 3. 参数适配与重命名： C2f 模块中若包含 V5 中没有的 `K` 参数（V5 中仅有 `G` 和 `E` 参数），需从 YOLOv8 代码中复制 `K` 参数相关定义，一并粘贴到 `commons.py`。 为避免覆盖 V5 原有模块，将新增的 C2f 模块重命名（如添加前缀 `C2F_`），确保命名符合项目规范，不与原有模块冲突。 #### 第二步：修改 models/yolo.py（设定传参细节） 1. 打开 `models/yolo.py` 文件，找到 `parse_model` 函数。 2. 传参适配：C2f 模块与 C3 模块的结构、参数完全一致，可直接参考 C3 模块的传参逻辑，在 `parse_model` 函数中添加 C2f 模块的传参处理： #### 第三步：修改 models/yolov5*.yaml（替换网络模块） 1. 复制现有模型配置文件（如 `models/yolov5s.yaml`），重命名为 `yolov5s_c2f.yaml`。 2. 打开 `yolov5s_c2f.yaml` 文件，修改 backbone 部分：将所有 `C3` 模块替换为新增的 `C2f` 模块。 #### 第四步：修改 train.py（训练时指定配置文件） 训练时需指定修改后的配置文件，确保模型加载 C2f 模块，核心代码与操作如下： 在train.py的参数解析部分，修改 cfg参数的默认值，使其指向修改后的C2f模型配置文件，避免训练时手动指定路径，代码如下： ```Python parser.add_argument(' cfg', type str, default ROOT / 'models/yolov5s_c2f.yaml', help 'model.yaml path') ```"},"/note/机器人项目笔记/YOLO/2.2-YOLOv5：Pycharm 与 AutoDL 远程连接.html":{"title":"YOLOv5：Pycharm 与 AutoDL 远程连接","content":"# YOLOv5：Pycharm 与 AutoDL 远程连接 ### 一、Pycharm 安装与基础配置 #### 1. 安装关键注意事项 **版本选择**：必须下载 **专业版**（Community 社区版不支持远程服务器连接功能），演示使用 2022.1.3 版本（需与实操一致可点击官网「Other Versions」选择历史版本，避免最新版兼容性问题）。 **安装选项**：勾选「Update context menu」和「Add open folder as project」，支持右键文件夹直接打开为 Pycharm 项目，提升操作便捷性。 **安装路径**：选择自定义路径（避免中文路径），按提示完成安装（无需额外复杂配置）。 #### 2. 本地项目打开与解释器配置 ##### （1）打开本地 YOLOv5 项目 右键 YOLOv5 项目文件夹（如 `yolov5 master`），选择「Open folder as PyCharm Project」，直接在 Pycharm 中加载项目。 初始状态下右下角会提示「No Python interpreter configured」，需手动配置环境。 ##### （2）配置 Conda 虚拟环境解释器 ```Python # 核心操作步骤（无代码，纯界面操作） 1. 打开 File → Settings → Project: yolov5 master → Python Interpreter 2. 点击右上角「+」号 → 选择「Conda Environment」→ 勾选「Existing environment」 3. 点击右侧「...」浏览，找到 yolov5 虚拟环境的 Python 可执行文件： 路径示例：C:\\Users\\用户名\\Miniconda3\\envs\\yolov5\\python.exe 4. 可选勾选「Make available to all projects」（所有项目共享该解释器） 5. 点击「OK」→ 再次「OK」，等待右下角「Updating Python interpreter」进程完成 ``` **备选方案（未识别 Conda 环境时）**：若上述步骤未找到环境，选择「System Interpreter」→ 手动浏览至 yolov5 环境的 `python.exe` 文件，完成配置。 **关键提醒**：配置后需等待「Updating indexes」进程结束（该进程运行时无法执行代码），仅「Updating Python interpreter」运行不影响操作。 #### 3. 终端设置（解决 Conda 环境激活问题） ##### （1）默认终端问题 Pycharm 默认终端为 PowerShell，无法正常激活 Conda 环境，导致 `pip install` 安装的包无法导入，需切换至 CMD 终端。 ##### （2）终端切换步骤 ```Python # 核心操作步骤（无代码，纯界面操作） 1. 打开 File → Settings → Tools → Terminal 2. 在「Application settings」→「Shell path」中，选择「Command Prompt (CMD)」 3. 点击「Apply」→「OK」，关闭当前终端并重新打开 4. 新终端将自动激活 yolov5 环境（前缀显示 (yolov5)） ``` **验证效果**：在新终端执行 `pip install jieba`，安装后在 Python 代码中 `import jieba` 无报错，说明配置成功。 ### 二、Pycharm 连接 AutoDL 远程服务器 #### 1. 前置准备 启动 AutoDL 服务器实例（参考第九章），获取「SSH 登录指令」和「登录密码」（AutoDL 控制台可直接复制）。 确保 Pycharm 为专业版，已完成本地环境配置。 #### 2. SSH 连接配置 ```Python # 核心操作步骤（无代码，纯界面操作） 1. 打开 File → Settings → Tools → SSH Configurations 2. 点击「+」号新建配置，填写以下信息： Host：从 AutoDL「SSH 登录指令」中提取（格式示例：region xxx.autodl.com） Port：从 SSH 登录指令中提取（格式示例：215583） Username：默认 root Password：点击「Set password」，粘贴 AutoDL 登录密码 3. 点击「Test Connection」，提示「Successfully connected」即为配置成功 4. 点击「OK」保存配置 ``` #### 3. 远程解释器配置（关键步骤） ```Python # 核心操作步骤（无代码，纯界面操作） 1. 打开 File → Settings → Project: yolov5 master → Python Interpreter 2. 点击「+」号 → 选择「SSH Interpreter」→ 勾选「Existing server configuration」 3. 选择上述新建的 AutoDL SSH 配置 → 点击「Next」 4. 配置远程 Python 解释器： Interpreter：浏览选择服务器上 Conda 环境的 Python 路径（示例：/root/miniconda3/bin/python） （可通过 AutoDL Jupyter Lab 终端执行 `which python` 查找路径） Sync folders（项目同步）： Local path：本地 YOLOv5 项目路径（自动识别，无需修改） Remote path：服务器上项目存储路径（建议新建文件夹：/root/yoloV5） 5. 点击「Finish」→「Apply」→「OK」，开始自动上传本地项目文件至服务器 ``` **上传提醒**：文件上传完成后将提示「286 个文件已传输」，需等待服务器端「Updating indexes」进程结束，再执行远程代码。 ### 三、远程服务器项目运行与文件同步 #### 1. 远程代码运行 配置完成后，右键 `detect.py` → 选择「Run detect」，代码将在 AutoDL 服务器上执行（终端显示远程执行路径）。 代码修改同步：本地修改代码后（如修改检测目标为 `bus.jpg`），Pycharm 会自动上传更新至服务器（可通过「Tools → Deployment → Automatic Upload」开启/关闭该功能）。 #### 2. 服务器文件下载（训练结果同步至本地） 服务器端训练结果（如 `runs/train/EXP4`）不会自动同步至本地，需手动下载： ```Python # 核心操作步骤（无代码，纯界面操作） 1. 打开 Tools → Deployment → Browse Remote Host（右侧弹出远程文件窗口） 2. 在远程窗口中找到目标文件夹（如 /root/优乐V5/runs/detect/EXP4） 3. 右键该文件夹 → 选择「Download from here」，指定本地保存路径 4. 等待下载完成，即可在本地查看服务器端检测/训练结果 ``` #### 3. 远程终端使用 打开 Pycharm 终端，右上角选择「Remote Python 3.8（AutoDL YOLOv5）」，即可进入服务器终端。 可直接在该终端执行服务器端命令（如 `python train.py`、`pip install 依赖包`），与 AutoDL Jupyter Lab 终端功能一致。 ### 四、多解释器切换（本地 UI 与远程训练兼顾） 服务器端未安装 PySide6 等 UI 依赖，无法运行可视化界面代码，需切换至本地解释器运行： ```Python # 核心操作步骤（无代码，纯界面操作） 1. 右键需运行的 UI 代码（如 base_ui.py）→ 选择「Edit Configurations」 2. 在「Python interpreter」中，选择本地 yolov5 环境解释器（而非远程解释器） 3. 点击「Apply」→「OK」，再点击「Run base_ui」，即可在本地启动可视化界面 ``` 优势：同一项目可灵活切换本地/远程解释器，远程用于训练（算力充足），本地用于 UI 演示（无需服务器依赖）。 ### 五、关键注意事项 1. **远程连接前提**：必须使用 Pycharm 专业版，社区版无 SSH 连接功能；AutoDL 服务器需处于「运行中」状态。 2. **环境一致性**：服务器端需提前配置 YOLOv5 环境（参考第九章），确保依赖包与本地一致，避免运行报错。 3. **文件同步方向**：本地→服务器自动同步（开启 Automatic Upload 后），服务器→本地需手动下载，训练结果需及时下载备份。 4. **成本控制**：远程训练完成后，及时在 AutoDL 控制台关闭服务器，避免长时间计费；关闭后 SSH 连接自动断开，再次启动需重新配置（IP/端口可能变化）。 5. **依赖安装**：服务器端缺失的依赖包，需在远程终端执行 `pip install` 安装，不可在本地终端安装（本地安装仅作用于本地环境）。 ### 六、整体流程回顾 1. Pycharm 安装（专业版+关键选项勾选）→ 本地项目打开→ 解释器配置（Conda/系统解释器）→ 终端切换（CMD 激活环境）。 2. AutoDL SSH 配置（Host/Port/账号密码）→ 远程解释器配置（服务器 Python 路径+项目同步）→ 本地文件自动上传。 3. 远程运行代码（训练/检测）→ 服务器结果手动下载→ 本地 UI 代码切换本地解释器运行。"},"/note/机器人项目笔记/YOLO/1.3-YOLOv5 数据集构建.html":{"title":"YOLOv5 数据集构建","content":"# YOLOv5 数据集构建 ## 一、数据收集核心要点 数据收集是数据集构建的前提，需覆盖目标场景的多样性，确保模型泛化能力。实操中主要分为图片和视频两种数据类型，适配不同原始数据场景。 **图片类型数据**：若直接获取图片（如网络爬取、相机拍摄），可直接用于后续标注，无需额外预处理。需注意图片分辨率适中，目标物体清晰，避免模糊、过度曝光等问题，同时保证图片数量覆盖不同角度、光线环境。 **视频类型数据**：若仅拥有视频素材，可通过 OpenCV 工具进行抽帧处理，将视频转化为连续图片。该方式能快速扩充数据集规模，尤其适合动态场景的数据收集（如实操中“死神VS火影”视频案例）。 关键原则：无论哪种数据类型，最终需将所有图片统一存放至指定文件夹（如 `images`），确保路径纯英文无空格，便于后续工具调用。 ## 二、OpenCV 视频抽帧实操 本章节以 40 秒左右的视频为例，演示如何通过 OpenCV 提取图片帧，解决颜色异常问题，并设置间隔帧减少冗余图片。 ### 1. 前期准备 1. 环境确认：确保已在 yolov5 虚拟环境中安装 OpenCV，若未安装可执行命令： `pip install opencv python matplotlib` （matplotlib 用于图片可视化，辅助调试抽帧效果） 2. 文件夹创建：新建名为 `images` 的文件夹，用于存放提取后的图片，建议与视频文件同级目录，便于路径管理。 ### 2. 基础抽帧代码（含颜色校正） OpenCV 读取视频默认采用 BGR 颜色通道，直接显示会出现颜色异常，需转换为 RGB 通道才能正常可视化。 ```Python # 导入依赖库 import cv2 import matplotlib.pyplot as plt # 打开视频文件（替换为自己的视频路径） video cv2.VideoCapture(\"./BVN.mp4\") # 读取一帧视频 ret, frame video.read() # 颜色通道转换：BGR → RGB（解决颜色异常问题） plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) ``` #### 代码关键说明 `cv2.VideoCapture()`：传入视频路径，创建视频读取对象，路径支持绝对路径（如 `C:/data/video.mp4`）和相对路径。 `ret` 与 `frame`：`ret` 为bool值，`True` 表示读取成功，`False` 表示视频读取结束；`frame` 为当前帧的图像数据。 `cv2.cvtColor()`：核心颜色转换函数，`cv2.COLOR_BGR2RGB` 为转换参数，确保图片显示颜色与原始视频一致。 ### 3. 完整抽帧代码（间隔帧设置） 视频帧速率通常为 30 帧/秒，直接抽帧会生成大量冗余图片，实操中设置间隔帧（如每隔 30 帧保存一张），平衡数据集规模与多样性。 ```Python video cv2.VideoCapture(\"./BVN.mp4\") num 0 save_step 30 while True: ret, frame video.read() if not ret: break num + 1 if num % save_step 0: cv2.imwrite(\"./images/\"+str(num)+\".jpg\", frame) ``` #### 实操验证 执行代码后，打开 `images` 文件夹，可看到按间隔帧保存的图片（如 30.jpg、60.jpg、90.jpg），说明抽帧逻辑正常。若需调整图片数量，可修改 `save_step` 参数（值越大，保存图片越少；值越小，保存图片越密集）。 ## 三、LabelImg 标注工具安装与使用 LabelImg 是一款轻量型可视化标注工具，支持 YOLO 格式标注文件生成，操作简单且适配 YOLOv5 模型需求，无需源码编译，可直接通过 pip 安装。 ### 1. 工具安装（yolov5 环境适配） 1. 打开 CMD，激活 yolov5 虚拟环境： `conda ` ctivate yolov5` 2. 执行安装命令，自动下载并安装 LabelImg： `pip install labelimg`说明：无需遵循官网源码编译流程，pip 安装可直接完成环境集成，国内源（前文配置的清华源）可加速下载。 ### 2. 工具启动与基础设置 #### 启动步骤 1. 在激活的 yolov5 环境中，直接输入命令启动 LabelImg： `labelimg` 2. 启动后将弹出标注窗口，需完成两项核心设置： **选择图片文件夹**：点击顶部「Open Dir」，选择前文抽帧后的 `images` 文件夹，加载所有待标注图片。 **选择标注结果保存文件夹**：点击顶部「Change Save Dir」，新建并选择 `labels` 文件夹（与 `images` 同级），用于存放标注生成的 TXT 文件。 **切换标注格式**：点击顶部「Save」下拉菜单，将格式从默认的「PascalVOC」切换为「YOLO」，确保生成的标注文件适配 YOLOv5。 **开启自动保存**：勾选顶部「View」→「Auto Save Mode」，标注完成后自动保存，无需手动点击保存按钮。 ### 3. 标注实操流程（以“daitu”“mingren”标注为例） 1. **创建标注框**： 快捷键`W`：快速创建矩形标注框（或右键选择「Create RectBox」）。 拖动鼠标框选图片中的目标物体（如“daitu”），松开后弹出标签输入框，输入目标类别名称（如“daitu”），点击「OK」完成单个目标标注。 2. **多目标标注**：重复步骤 1，框选图片中的其他目标（如“mingren”），输入对应类别名称，完成单张图片全目标标注。 3. **批量标注**： 快捷键 `D`：切换至下一张图片。 快捷键 `A`：切换至上一张图片，便于修改标注内容。 按上述流程逐张标注，标注过程中可通过「View」→「Display Labels」查看已标注框，校验标注准确性。 ### 4. 标注结果解析 批量标注完成后，打开 `labels` 文件夹，将生成两类文件，与 `images`文件夹中的图片一一对应，为后续模型训练提供数据支撑。 第一列（0/1）：类别索引，对应 `classes.txt` 中的类别顺序（0 为“daitu”，1 为“mingren”）。 **TXT 标注文件**： 每个图片对应一个同名 TXT 文件，文件内容格式如下（以“daitu”和“mingren”为例） **classes.txt 文件**：自动生成的类别清单，记录所有标注类别名称（如“daitu”“mingren”），顺序与 TXT 文件中的类别索引对应，不可随意修改顺序。"},"/note/机器人项目笔记/YOLO/2.3-YOLOv5 VS Code 与 AutoDL 远程连接.html":{"title":"YOLOv5 VS Code 与 AutoDL 远程连接","content":"# YOLOv5 VS Code 与 AutoDL 远程连接 ### 一、VS Code 安装与基础配置 #### 1. 安装关键步骤 **下载渠道**：访问 VS Code 官网（[code.visualstudio.com](https://code.visualstudio.com)），点击「Download for Windows」下载安装包（无需区分版本，默认最新版即可）。 **安装选项**：勾选以下核心选项（提升后续操作便捷性）： 创建桌面快捷方式 通过 Code 打开（添加到上下文菜单）：支持右键文件夹直接打开为 VS Code 项目 其他推荐组件（默认勾选即可） **安装流程**：同意协议→选择安装路径→点击「安装」，等待完成（无需额外复杂配置）。 #### 2. 基础个性化配置 **语言切换（中文）**： 1. 打开 VS Code，左侧点击「扩展」（图标：🧩），搜索「Chinese (Simplified)」插件。 2. 点击「安装」，安装完成后重启 VS Code，自动切换为中文界面。 **主题选择**：重启后可根据喜好选择界面主题（不影响功能，仅个性化展示）。 #### 3. Python 核心插件安装 VS Code 默认仅为文本编辑器，需安装插件支持 Python 开发： 1. 左侧点击「扩展」，搜索「Python」（作者：Microsoft），点击「安装」。 2. 插件自动附带安装 Jupyter、Python 调试等相关依赖，无需额外手动安装。 3. 安装完成后，左侧将显示「Get Started with Python Development」，表明插件配置成功。 ### 二、本地 YOLOv5 项目配置（解释器+终端） #### 1. 打开本地项目 **两种方式**： 1. 右键 YOLOv5 项目文件夹（如 `yolov5 master`），选择「通过 Code 打开」，直接加载项目。 2. 打开 VS Code→点击「文件」→「打开文件夹」，选择项目路径加载。 **安全提示**：首次打开将提示「是否信任此文件夹」，点击「信任」即可正常编辑（避免恶意脚本风险）。 #### 2. 解释器配置（切换至 yolov5 虚拟环境） ```Python # 核心操作步骤（无代码，纯界面操作） 1. 打开项目后，右下角显示当前解释器（默认可能为 Conda base 环境，如「3.8.15 (base)」）。 2. 点击右下角解释器名称，弹出环境列表，选择 yolov5 虚拟环境（如「3.8.16 (yolov5)」）。 3. 若未找到 yolov5 环境： 点击「输入解释器路径」→「浏览」，找到 yolov5 环境的 Python 可执行文件： 路径示例：C:\\Users\\用户名\\Miniconda3\\envs\\yolov5\\python.exe 选择后自动切换，右下角显示「3.8.16 (yolov5)」即为配置成功。 ``` **验证效果**：右键 `detect.py`→「运行Python文件」，终端输出检测结果，且结果保存至 `runs/detect/EXPx`，说明解释器配置成功。 #### 3. 终端设置（解决 Conda 环境激活问题） ##### （1）默认终端问题 VS Code 默认终端为 PowerShell，无法正常激活 Conda 环境（前缀无 `(yolov5)` 标识），导致 `pip install` 安装的依赖无法导入，需切换至 CMD 终端。 ##### （2）终端切换与默认配置 ```Python # 核心操作步骤（无代码，纯界面操作） 1. 打开终端：点击「终端」→「新建终端」（默认启动 PowerShell，前缀显示「PS」）。 2. 临时切换 CMD：点击终端右上角「▼」→「Command Prompt」，新建 CMD 终端（自动激活 yolov5 环境，前缀显示 `(yolov5)`）。 3. 设置 CMD 为默认终端（永久生效）： 点击终端右上角「▼」→「选择默认配置文件」→「Command Prompt」。 关闭当前所有终端，重新新建终端，默认启动 CMD 且自动激活 yolov5 环境。 ``` **验证效果**：在 CMD 终端执行 `pip install jieba`，安装后在 Python 代码中 `import jieba` 无报错，说明配置成功。 ### 三、VS Code 远程连接 AutoDL 服务器 #### 1. 前置准备 启动 AutoDL 服务器实例（参考第九章），获取「SSH 登录指令」和「登录密码」（AutoDL 控制台可直接复制）。 确保 VS Code 已安装「Remote SSH」插件。 #### 2. 安装 Remote SSH 插件 1. 左侧点击「扩展」，搜索「Remote SSH」（作者：Microsoft），点击「安装」。 2. 安装完成后，左下角将显示「远程窗口」图标，表明插件就绪。 #### 3. SSH 连接配置 ```Python # 核心操作步骤（无代码，纯界面操作） 1. 点击左下角「远程窗口」图标→「连接到主机」→「添加新的 SSH 主机」。 2. 粘贴 AutoDL 「SSH 登录指令」（示例：ssh root@region xxx.autodl.com p 215583），回车。 3. 选择 SSH 配置文件保存路径（默认选择第一个即可），点击「保存」。 4. 再次点击「连接到主机」，选择刚添加的 SSH 主机，弹出选择系统类型，点击「Linux」。 5. 输入 AutoDL 登录密码（可从控制台复制「登录密码」），回车。 6. 提示「成功连接」后，远程连接建立完成。 ``` #### 4. 打开服务器项目与运行代码 ```Python # 核心操作步骤（无代码，纯界面操作） 1. 连接成功后，点击「打开文件夹」→ 选择服务器上的项目路径（如 `/root/yolov5 master`）→ 输入密码确认。 2. 服务器项目加载完成后，需安装 Python 扩展到服务器： 左侧点击「扩展」→ 找到「Python」插件→ 点击「在 SSH: xxx 上安装」（避免本地插件未同步）。 3. 运行远程代码： 右键 `detect.py`→「运行Python文件」，终端将在服务器上执行代码，检测结果保存至服务器 `runs/detect/EXPx`。 若插件安装失败（服务器网络问题），可在远程终端执行命令运行： ```bash python detect.py ``` ``` ### 四、远程文件操作与 Jupyter 使用 #### 1. 服务器文件下载（训练结果同步至本地） 服务器端训练/检测结果不会自动同步至本地，需手动下载： ```Python # 核心操作步骤（无代码，纯界面操作） 1. 在 VS Code 左侧「资源管理器」中，找到服务器上的目标文件夹（如 `/root/yolov5 master/runs/train/EXP4`）。 2. 右键该文件夹→ 选择「下载」，指定本地保存路径（如桌面）。 3. 等待下载完成，即可在本地查看服务器端结果（文件夹结构与服务器一致）。 ``` #### 2. 本地文件上传至服务器 直接拖拽本地文件（如数据集、配置文件）至 VS Code 左侧服务器项目的目标路径，即可自动上传。 上传完成后，服务器端对应路径将显示该文件，无需额外操作。 #### 3. Jupyter Notebook 使用（服务器/本地均可） ```Python # 核心操作步骤（无代码，纯界面操作） 1. 新建 Jupyter 文件：点击「文件」→「新建文件」→ 选择「Jupyter Notebook」→ 命名为 `demo.ipynb`。 2. 选择内核：点击文件右上角「选择内核」→ 选择 yolov5 环境（本地/远程均可，远程需已连接服务器）。 3. 运行代码块：输入代码（如 `print(\"Hello YOLOv5\")`），按 `Shift+Enter` 运行，正常输出即为配置成功。 4. 若内核选择失败（新环境）：在终端执行 `pip install jupyter lab`，安装后重新选择内核。 ``` ### 五、整体流程回顾 1. VS Code 安装（勾选关键选项）→ 中文语言+Python 插件安装→ 本地项目打开→ 解释器切换（yolov5 环境）→ 终端切换（CMD）。 2. Remote SSH 插件安装→ SSH 连接配置（AutoDL 主机/端口/密码）→ 服务器项目加载→ 远程 Python 插件安装→ 代码运行。 3. 服务器文件下载/本地文件上传→ Jupyter Notebook 配置与使用。"},"/note/机器人项目笔记/YOLO/4.2-基于Flask的YOLOv5项目部署.html":{"title":"基于Flask的YOLOv5项目部署","content":"# 基于Flask的YOLOv5项目部署 ## 一、前置说明与核心准备 ### 1. 部署核心目标 通过Flask框架搭建REST API服务，实现YOLOv5模型的远程调用，支持两种预测方式：基于文件的预测（官方已提供示例）、基于图像字节流的预测（适配内存中读取的图片/视频帧），核心依赖YOLOv5的hub加载功能与OpenCV的图像编码解码工具。 ### 2. 环境依赖 基础环境：Python 3.8.x、YOLOv5 项目环境（已配置PyTorch、OpenCV等依赖） 核心框架：Flask（用于搭建API服务） 工具库：OpenCV（图像编码解码）、numpy（字节流转换） 模型要求：YOLOv5系列模型（示例使用YOLOv5s，加载机制适配本地模型） ### 3. 官方示例位置 YOLOv5项目中已内置Flask部署示例，路径为：`utils/flask_rest_api`，核心文件为REST API相关代码，可直接复制到项目根目录修改使用，无需从零编写。 ## 二、核心操作：代码迁移与修改 ### 第一步：代码迁移 1. 将 `utils/flask_rest_api` 目录下的除README.md所有文件，复制到YOLOv5项目根目录（必须放在根目录，因需调用 `hub.load` 加载模型，需明确路径）。 2. 迁移目的：确保模型加载时能找到 `hubconf.py` 文件，避免因路径错误导致加载失败。 ### 第二步：修改REST API代码（适配本地模型） 核心修改点：替换模型加载方式为本地加载，保留其他核心逻辑。 #### 1. 模型加载部分修改 ```Python # 本地加载YOLOv5模型，适配自定义模型 # ./ 表示当前根目录（需包含hubconf.py，否则无法加载模型） # m 为模型名称，source \"local\" 指定本地加载，无需联网 models[m] torch.hub.load(\"./\", m, source \"local\") ``` 修改说明：`source \"local\"` 指定加载本地模型，无需联网；`path` 为本地模型路径（可替换为自定义训练的 `best.pt`），YOLOv5s模型可直接加载，无需额外配置。 #### 2. 新增基于图像字节流的预测接口（适配内存中图片/视频帧） 需添加对POST请求中`data`字段的处理，使用OpenCV完成字节流与图像的转换（代码从原文提取，无修改）： ```Python import numpy as np import cv2 # 读取请求中的图像字节流数据（适配内存中图片/视频帧） if request.data: # 字节流转换为numpy数组（uint8格式，对应图像像素范围） # 解码numpy数组为OpenCV图像，cv2.IMREAD_COLOR表示读取彩色图像 img cv2.imdecode(np.frombuffer(request.data, np.uint8), cv2.IMREAD_COLOR) # 检查模型是否已加载（避免模型未初始化报错） if model in models: # 调用模型进行预测 results models[model](img) # 以JSON格式返回预测结果（XYXY坐标，适配前端/其他服务调用） return results.pandas().xyxy[0].to_json(orient \"records\") ``` #### 3. 可选优化：返回标注后的图像 若需服务返回带检测框的图像，修改接口返回逻辑（代码从原文提取，无修改）： ```Python # 读取请求中的图像字节流数据 if request.data: # 字节流转numpy数组，再解码为OpenCV彩色图像 img cv2.imdecode(np.frombuffer(request.data, np.uint8), cv2.IMREAD_COLOR) # 检查模型是否已加载 if model in models: # 调用模型预测，可添加size 320参数缩小图像，提升推理速度 results models[model](img) # reduce size 320 for faster inference # 生成带检测框、类别标签的标注图像（render()返回标注后图像列表，取第一个元素） results results.render()[0] # 编码标注图像为jpg格式字节流，返回给请求方（适配图像展示场景） return cv2.imencode('.jpg', results)[1].tobytes() ``` ## 三、服务启动与请求测试 ### 1. 两种请求方式（需启动服务后测试）修改example_request.py代码 所有请求需在Flask服务启动后执行，以下为两种核心请求方式及接口测试代码： #### （1）基于文件的请求 ```Python # 基于文件的请求示例：需修改为本地实际图片路径 # 路径可写相对路径（如当前示例，基于YOLOv5根目录）或绝对路径 image_path \"data/image/test.jpg\" ``` #### （2）基于图像字节流的请求 ```Python import pprint # 用于格式化打印预测结果，便于查看 import cv2 # 用于读取、处理图像 import requests# 用于发送HTTP请求，调用Flask接口 # Flask服务的接口地址（对应模型预测接口，需与服务端一致） DETECTION_URL \"http://localhost:5000/v1/object detection/yolov5s\" # 本地测试图像路径（需替换为自己的图像路径） IMAGE \"data/images/zidane.jpg\" # # 备选：以文件读取方式获取图像字节流（注释可保留，按需切换） # with open(IMAGE, \"rb\") as f: # image_data f.read() # 读取本地图像（OpenCV默认读取为BGR格式） img cv2.imread(IMAGE) # 转换图像通道：BGR转RGB（适配YOLOv5模型输入要求） img cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # 编码图像为jpg格式字节流（适配接口的字节流请求格式） img cv2.imencode('.jpg', img)[1].tobytes() # 发送POST请求，传入图像字节流，获取预测结果（JSON格式） response requests.post(DETECTION_URL, data img).json() # 格式化打印预测结果（类别、坐标、置信度等信息） pprint.pprint(response) ``` #### （3）测试返回图像接口 ```Python # YOLOv5 🚀 by Ultralytics, GPL 3.0 license \"\"\" Perform test request 测试返回标注图像的接口，获取带检测框的图像并展示 \"\"\" import pprint import cv2 # 图像读取、解码处理 import requests# 发送HTTP请求 import numpy as np # 字节流与图像转换 import matplotlib.pyplot as plt # 展示标注后的图像 # Flask服务的预测接口地址（需与服务端一致） DETECTION_URL \"http://localhost:5000/v1/object detection/yolov5s\" # 本地测试图像路径（按需替换） IMAGE \"data/images/zidane.jpg\" # 读取本地图像（BGR格式） img cv2.imread(IMAGE) # BGR转RGB，适配YOLOv5模型输入 img cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # 编码为jpg字节流，用于接口请求 img cv2.imencode('.jpg', img)[1].tobytes() # 发送POST请求，传入字节流，获取返回的标注图像字节流 response requests.post(DETECTION_URL, data img) # 解码返回的字节流，转换为OpenCV可处理的图像格式 img cv2.imdecode(np.frombuffer(response.content, np.uint8), cv2.IMREAD_COLOR) # 用matplotlib展示标注后的图像（含检测框、类别标签） plt.imshow(img) plt.show() ``` ### 2. 启动Flask服务 ```Bash # 启动Flask服务，运行restapi.py脚本（需在YOLOv5根目录执行） # 服务默认监听5000端口，启动后可通过接口发送预测请求 python restapi.py ``` 启动成功提示：服务运行在本地5000端口（`http://0.0.0.0:5000`），按`Ctrl+C`可退出服务，启动后即可执行上述请求测试。"},"/note/机器人项目笔记/YOLO/3.3-YOLOv5 引入 SE 注意力机制.html":{"title":"YOLOv5 引入 SE 注意力机制","content":"# YOLOv5 引入 SE 注意力机制 ### 一、前置准备：借鉴SE模块开源代码 #### 1. 代码借鉴来源 ```Plain Text # SE注意力机制开源代码地址 https://github.com/Zhugekongan/Attention mechanism implementation ``` #### 2. 代码提取与核心分析 1. 访问上述 GitHub 地址，定位 `model` 目录下的 `SE block` 代码，复制完整模块实现（含导包语句）。 2. SE 模块核心功能：通过“平均池化→通道压缩→通道恢复→激活→与原始特征相乘”，强化关键通道特征，抑制无关信息。 3. SE 模块关键特性（与 C2f/C3 差异）： 输入通道数 输出通道数（先压缩再恢复，通道数不变）。 仅需 2 个参数：`in_channels`（输入通道数）、`ratio`（通道压缩比，决定中间隐层通道数）。 中间隐层通道数 `in_channels // ratio`（压缩比可自定义，如 2）。 ### 二、引入SE机制全流程（4步完成，核心是层编号调整） 核心修改顺序： `models/commons.py` → 加入 SE 模块代码 → `models/yolov5*.yaml` → 修改配置文件（加 SE 层+调整层编号） → `models/yolo.py` → 配置 SE 传参细节 → `train.py` → 训练时指定自定义配置文件 #### 第一步：修改 models/[commons.py](commons.py)（添加SE模块） 1. 打开 YOLOv5 项目的 `models/commons.py` 文件。 2. 复制 SE 模块代码（含导包语句），粘贴到文件中（建议放在 C2f/C3 模块之前，避免冲突）： 3. 确保导包语句 `import torch.nn.functional as F` 已添加，避免“F未定义”报错。 #### 第二步：修改 models/yolov5*.yaml（添加SE层+调整层编号） 1. 复制现有模型配置文件（如 `models/yolov5s.yaml`），重命名为 `yolov5s_se.yaml`（标识含 SE 机制的模型）。 2. 添加 SE 层：选择在 backbone 最后一层加入 SE 模块（原文推荐位置），修改 backbone 部分配置： ```YAML # 修改后的 backbone 配置（仅展示新增 SE 层部分，其余保持不变） backbone: [[ 1, 1, Conv, [64, 6, 2, 2]], [ 1, 1, Conv, [128, 3, 2]], [ 1, 3, C3, [128]], [ 1, 1, Conv, [256, 3, 2]], [ 1, 6, C3, [256]], [ 1, 1, Conv, [512, 3, 2]], [ 1, 9, C3, [512]], [ 1, 1, Conv, [1024, 3, 2]], [ 1, 3, C3, [1024]], [ 1, 1, SPPF, [1024, 5]], [ 1, 1, SE, [1024, 2]], # 新增 SE 层：输入通道1024，压缩比2 ] ``` 1. 关键操作：调整后续层的 `from` 参数（新增 SE 层会改变原有层编号）： 新增 SE 层为第 10 层（原 SPPF 为第 9 层），因此原第 10 层及以后的层编号均 +1。 重点调整 head 部分依赖 backbone 层的 `from` 参数： 原第 19 层（现第 20 层）：输入从「11, 14」改为「12, 15」（均 +1）。 原 Detect 层输入「17, 20, 23」改为「18, 21, 24」（均 +1）。 核心原则：新增层之后，所有引用该层及后续层的 `from` 参数，均需按新增层数累加调整。 #### 第三步：修改 models/[yolo.py](yolo.py)（配置SE模块传参细节） 1. 打开 `models/yolo.py` 文件，找到 `parse_model` 函数（模型解析核心函数）。 2. 在 `parse_model` 中添加 SE 模块的传参逻辑（新增 `elif` 分支，参考 C3/C2f 传参格式）： ```Python elif m is SE: # 新增 SE 模块传参逻辑 c1 ch[f] # SE 输入通道 上一层输出通道 c2 args[0] # SE 输出通道 输入通道（通道数不变） # 通道数按 width_multiple 缩放，且确保可被8整除 if c2 ! no: # if not output c2 make_divisible(c2 * gw, 8) args [c1, *args[1:]] # 传入 in_channels 和 ratio 参数 ``` 1. 关键说明： SE 模块无需 `n`（重复次数），默认 `n 1`，无需额外处理。 必须通过 `make_divisible` 确保通道数可被 8 整除，适配 GPU 加速。 `args` 仅保留 `in_channels` 和 `ratio`（从配置文件传入），符合 SE 模块初始化参数要求。 #### 第四步：修改 [train.py](train.py)（训练时指定配置文件） 1. 打开 `train.py`，修改 ` cfg` 参数的默认值，指向含 SE 机制的配置文件： ```Python # 从原文提取的修改后代码（无任何更改） parser.add_argument(' cfg', type str, default ROOT / 'models/yolov5s_se.yaml', help 'model.yaml path') ```"},"/note/机器人项目笔记/YOLO/1.5-YOLOv5 Pyside6 可视化界面.html":{"title":"YOLOv5 Pyside6 可视化界面","content":"# YOLOv5 Pyside6 可视化界面 ### 一、可视化界面环境安装 需安装 Pyside6 框架、QT Designer 设计工具及 VS Code 辅助插件，环境基于前文 yolov5 虚拟环境搭建。 #### 1. 核心依赖安装 在激活的 yolov5 虚拟环境中执行以下命令： ```Bash # 安装 Pyside6（含 QT Designer） pip install pyside6 # 安装 jupyter lab（辅助模型调用验证） pip install jupyter lab ``` #### 2. QT Designer 定位与快捷方式创建 QT Designer 随 Pyside6 一并安装，需找到其路径并创建桌面快捷方式： 1. 查找 Python 安装路径：打开 CMD，输入 `where python`，获取当前 yolov5 环境的 Python 路径（如 `C:\\Miniconda3\\envs\\yolov5\\python.exe`）。 2. 定位 QT Designer：进入 Python 路径下的 `C:\\Users\\12804\\miniconda3\\envs\\yolov5\\Lib\\site packages\\PySide6` 文件夹，找到 `designer.exe` 文件。 3. 创建快捷方式：右键 `designer.exe`，选择「发送到」→「桌面快捷方式」，便于后续快速启动。 #### 3. VS Code 插件安装（UI 编译支持） 打开 VS Code，在左侧扩展栏搜索「QT for Python」，点击安装，用于将 QT Designer 生成的 `.ui` 文件编译为 Python 代码。 ### 二、QT Designer 界面设计（拖拽式可视化） #### 1. 新建 UI 项目 1. 双击桌面 `designer.exe` 启动工具，选择「Main Window」→ 点击「创建」，生成空白窗口。 2. 移除默认菜单栏（无需保留），按以下规划设计界面布局。 #### 2. 界面组件拖拽与布局 #### 3. 保存 UI 文件 按 `Ctrl+S` 保存文件，路径选择 YOLOv5 根目录，文件名为 `m` `ain` `_window.ui`（后缀为 `.ui`，不可修改）。 ### 三、UI 文件编译（.ui → .py） 通过 VS Code 插件将设计好的 `.ui` 文件编译为 Python 代码，便于后续逻辑编写： 1. 用 VS Code 打开 YOLOv5 根目录，找到 `main_window.ui` 文件。 2. 右键点击该文件，选择「Compile QT UI File」，自动生成 `main_window` `_ui` `.py` 文件（含界面组件的 Python 定义）。 ### 四、核心代码实现（图片/视频检测逻辑） 新建 Python 文件 `base_ui.py`，导入编译后的 UI 模块，实现信号绑定、文件读取、模型调用、结果显示等逻辑，代码完全复刻实操演示： ```Python # 导入核心依赖 import sys import torch import cv2 # 导入 PySide6 相关模块 from PySide6.QtWidgets import QMainWindow, QApplication, QFileDialog from PySide6.QtGui import QPixmap,QImage # 导入编译后的 UI 模块（同步修改为新编译文件名） from main_window_ui import Ui_MainWindow from PySide6.QtCore import QTimer def convert2QImage(img): \"\"\"将 OpenCV 图像转换为 QImage 格式\"\"\" # 获取图像的高度、宽度和通道数 height, width, channel img.shape # 创建并返回QImage对象，使用RGB888格式 return QImage(img, width, height, width*channel, QImage.Format_RGB888) class MainWindow(QMainWindow, Ui_MainWindow): def __init__(self): # 调用父类初始化方法 super(MainWindow, self).__init__() # 初始化 UI 界面 self.setupUi(self) # 加载YOLOv5模型：从本地目录加载自定义权重文件 self.model torch.hub.load(\"./\",\"custom\", path \"runs/train/exp/weights/best.pt\", source \"local\") # 创建定时器对象，用于视频帧处理 self.timer QTimer() # 设置定时器间隔（1毫秒，实际视频处理速度受性能限制） self.timer.setInterval(1) # 初始化视频捕获对象 self.video None # 绑定信号与槽函数 self.bind_slots() def image_pred(self,file_path): # 图像预测逻辑 # 使用模型对图像进行推理 results self.model(file_path) # 渲染并获取处理后的图像（第一张） image results.render()[0] # 显示第一张图像的结果 # 将处理后的图像转换为QImage格式 return convert2QImage(image) def open_image(self): # 停止定时器（防止与视频处理冲突） self.timer.stop() # 打开文件对话框选择图像文件，默认路径为训练集图像目录 file_path QFileDialog.getOpenFileName(self, dir \"./datasets/images/train\", filter \"*.png *.jpg *.jpeg\") # 如果用户选择了文件（路径不为空） if file_path[0]: # 获取文件路径字符串 file_path file_path[0] # 对图像进行预测并获取处理后的QImage qimage self.image_pred(file_path) # 在输入显示区域显示原始图像 self.input.setPixmap(QPixmap(file_path)) # 在输出显示区域显示处理后的图像 self.output.setPixmap(QPixmap.fromImage(qimage)) def video_pred(self): # 视频预测逻辑（由定时器周期性调用） # 读取视频的下一帧 ret, frame self.video.read() # 如果读取失败（视频结束） if not ret: # 停止定时器 self.timer.stop() else: # 将BGR格式转换为RGB格式 frame cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # 在输入显示区域显示当前帧 self.input.setPixmap(QPixmap.fromImage(convert2QImage(frame))) # 使用模型对当前帧进行推理 results self.model(frame) # 渲染并获取处理后的图像 image results.render()[0] # 在输出显示区域显示处理后的帧 self.output.setPixmap(QPixmap.fromImage(convert2QImage(image))) def open_video(self): # 打开视频文件 # 打开文件对话框选择视频文件 file_path QFileDialog.getOpenFileName(self, dir \"./datasets\", filter \"*.mp4\") # 如果用户选择了文件 if file_path[0]: # 获取文件路径字符串 file_path file_path[0] # 创建视频捕获对象 self.video cv2.VideoCapture(file_path) # 启动定时器，开始视频处理循环 self.timer.start() def bind_slots(self): # 绑定按钮点击事件到相应的槽函数 # 图像检测按钮 > 打开图像方法 self.det_image.clicked.connect(self.open_image) # 视频检测按钮 > 打开视频方法 self.det_video.clicked.connect(self.open_video) # 定时器超时信号 > 视频预测方法 self.timer.timeout.connect(self.video_pred) # 程序入口 if __name__ \"__main__\": # 创建QApplication实例（GUI应用程序必需） app QApplication(sys.argv) # 创建主窗口实例 window MainWindow() # 显示主窗口 window.show() # 启动应用程序事件循环 app.exec() ``` ### 五、关键功能逻辑解析 #### 1. 核心组件与信号绑定 信号与槽机制：通过`clicked.connect()` 将按钮点击事件与函数绑定（如代码中 `det_image` 绑定 `open_image`、`det_video` 绑定 `open_video`）。 计时器：解决 QT 界面阻塞问题，视频检测时通过 `QTimer` 定时读取帧，设置 1 毫秒间隔，避免循环阻塞界面刷新，同时初始化 `self.video None` 防止资源残留。 #### 2. 图片检测核心流程 1. 文件选择：通过 `QFileDialog.getOpenFileName()` 打开图片，默认路径为 `./datasets/images/train`（同步代码中 datasets 复数格式），过滤 `.png/.jpg/.jpeg` 格式。 2. 模型检测：调用训练好的 `best.pt` 模型（路径为 `runs/train/exp/weights/best.pt`），通过 `results.render()` 提取检测结果（array 格式）。 3. 格式转换：代码中 `convert2QImage()` 函数实现格式转换，先适配图像宽高通道，直接将处理后图像转为 QImage 格式，确保 QT 正常显示。 #### 3. 视频检测核心流程 1. 视频初始化：`cv2.VideoCapture()` 读取视频文件，默认打开路径为 `./datasets`（复数格式），计时器启动后按 1 毫秒间隔读取帧，保证流畅播放。 2. 帧处理：循环读取视频帧，先将 BGR 格式转为 RGB 格式，再执行「原始帧显示→模型检测→检测结果显示」流程，逻辑与图片检测一致。 3. 终止条件：视频读取完毕（`ret False`）时，停止计时器释放资源，避免后台占用。 #### 4. 冲突优化 图片检测时调用 `self.timer.stop()`，停止视频检测计时器，避免两者同时运行冲突，同时清空之前的视频资源。 计时器间隔固定为 1 毫秒（代码中已设置），无需额外调整，可直接实现视频流畅播放。 ### 六、界面运行与测试 #### 1. 运行程序 在 yolov5 虚拟环境中，进入代码所在目录，执行（同步修改为新文件名）： ```Bash python base_ui.py ``` #### 2. 功能测试 1. 图片检测：点击「图片检测」按钮→ 选择 `datasets/images/train`（复数格式）下的图片（如 30.jpg）→ 左侧显示原始图片，右侧显示检测结果（标注daitu、mingren）。 2. 视频检测：点击「视频检测」按钮→ 选择 `datasets`（复数格式）下的 MP4 视频（如 BVN.mp4）→ 左侧播放原始视频，右侧实时显示检测结果。 3. 终止操作：视频检测时按 `Ctrl+C` 终止程序（不可直接关闭窗口），避免资源泄露。 ### 七、常见问题与注意事项 1. 界面无响应：视频检测时未使用计时器，或未初始化 `self.video`，导致循环阻塞 QT 事件队列，需确保 `video_pred` 由 `QTimer` 触发，且初始化视频资源。 2. 图片显示异常：检查`convert2QImage()` 函数参数，确保图像宽、高、通道数适配，且格式设为 `QImage.Format_RGB888`。 3. 模型加载失败：检查 `best.pt` 路径是否正确（需与代码一致，为 `runs/train/exp/weights/best.pt`），同时确认模型与 YOLOv5 版本兼容。 4. 依赖冲突：安装 Pyside6 后若报错，执行`pip uninstall pywin32` 卸载冲突包，重新启动程序即可。 ### 八、整体流程回顾 1. 环境搭建：安装 Pyside6、QT Designer 及辅助工具，创建设计工具快捷方式，确保环境与 yolov5 虚拟环境一致。 2. UI 设计：通过 QT Designer 拖拽组件、设置属性，保存为 `main_window.ui` 文件，编译为 `main_window_ui.py` 模块（同步代码导入路径）。 3. 逻辑编写：新建`base_ui.py` 文件，导入编译后的 UI 模块，实现文件选择、模型调用、格式转换、视频帧处理等核心功能，通过信号与槽绑定 `det_image`、`det_video` 按钮事件。 4. 测试优化：运行 `python base_ui.py` 测试功能，确保图片/视频检测流畅，无界面阻塞、格式异常等问题。"},"/note/机器人项目笔记/YOLO/3.1-YOLOv5 模型结构与构建原理.html":{"title":"YOLOv5 模型结构与构建原理","content":"# YOLOv5 模型结构与构建原理 ### 一、模型结构的定义位置与核心文件 YOLOv5 模型结构并非硬编码在业务逻辑中，而是统一配置在项目的 `models` 文件夹下的 YAML 文件中，不同模型（S/M/L/X）对应不同配置文件，结构逻辑一致，仅参数存在差异。 #### 1. 核心文件说明 `models/yolov5s.yaml`（以 S 模型为例）：存储模型核心结构配置，包括类别数、缩放系数、锚框、主干网络（backbone）、头部结构（head）等关键信息。 `models/yolo.py`：模型构建核心逻辑，负责解析 YAML 配置并实例化网络模块。 `models/commons.py`：定义网络基础模块（如 Conv、C3、SPPF 等），供 YAML 配置调用。 #### 2. YAML 配置文件核心内容解析（yolov5s.yaml） ```yaml # YOLOv5s 模型配置文件 (假设输入为640x640) # 从原文提取的核心配置（无修改） nc: 80 # 目标类别数，默认80（COCO数据集） depth_multiple: 0.33 # 深度缩放系数（控制模块重复次数） width_multiple: 0.50 # 宽度缩放系数（控制通道数） anchors: # 锚框，3个特征图各3组锚框 [10,13, 16,30, 33,23] [30,61, 62,45, 59,119] [116,90, 156,198, 373,326] # 主干网络（backbone） 特征提取部分 backbone: # [从哪层输入, 重复次数, 模块类型, [参数]] [[ 1, 1, Conv, [64, 6, 2, 2]], # 第0层: Conv, 输出 1*64*320*320 (640/2 320) [ 1, 1, Conv, [128, 3, 2]], # 第1层: Conv, 输出 1*128*160*160 (320/2 160) [ 1, 3, C3, [128]], # 第2层: C3, 输出 1*128*160*160 (尺寸不变) [ 1, 1, Conv, [256, 3, 2]], # 第3层: Conv, 输出 1*256*80*80 (160/2 80) [ 1, 6, C3, [256]], # 第4层: C3, 输出 1*256*80*80 (尺寸不变) [ 1, 1, Conv, [512, 3, 2]], # 第5层: Conv, 输出 1*512*40*40 (80/2 40) [ 1, 9, C3, [512]], # 第6层: C3, 输出 1*512*40*40 (尺寸不变) [ 1, 1, Conv, [1024, 3, 2]], # 第7层: Conv, 输出 1*1024*20*20 (40/2 20) [ 1, 3, C3, [1024]], # 第8层: C3, 输出 1*1024*20*20 (尺寸不变) [ 1, 1, SPPF, [1024, 5]], # 第9层: SPPF, 输出 1*1024*20*20 (尺寸不变) ] # 头部结构（head） 多尺度特征融合和检测部分 head: [[ 1, 1, Conv, [512, 1, 1]], # 第10层: Conv(1x1), 输出 1*512*20*20 (尺寸不变) [ 1, 1, nn.Upsample, [None, 2, 'nearest']], # 第11层: 上采样2倍, 输出 1*512*40*40 [[ 1, 6], 1, Concat, [1]], # 第12层: 拼接第11层和第6层, 输出 1*1024*40*40 [ 1, 3, C3, [512, False]], # 第13层: C3, 输出 1*512*40*40 (尺寸不变) [ 1, 1, Conv, [256, 1, 1]], # 第14层: Conv(1x1), 输出 1*256*40*40 (尺寸不变) [ 1, 1, nn.Upsample, [None, 2, 'nearest']], # 第15层: 上采样2倍, 输出 1*256*80*80 [[ 1, 4], 1, Concat, [1]], # 第16层: 拼接第15层和第4层, 输出 1*512*80*80 [ 1, 3, C3, [256, False]], # 第17层: C3, 输出 1*256*80*80 (尺寸不变) [ 1, 1, Conv, [256, 3, 2]], # 第18层: Conv(3x3,stride 2), 输出 1*256*40*40 (80/2 40) [[ 1, 14], 1, Concat, [1]], # 第19层: 拼接第18层和第14层, 输出 1*512*40*40 [ 1, 3, C3, [512, False]], # 第20层: C3, 输出 1*512*40*40 (尺寸不变) [ 1, 1, Conv, [512, 3, 2]], # 第21层: Conv(3x3,stride 2), 输出 1*512*20*20 (40/2 20) [[ 1, 10], 1, Concat, [1]], # 第22层: 拼接第21层和第10层, 输出 1*1024*20*20 [ 1, 3, C3, [1024, False]], # 第23层: C3, 输出 1*1024*20*20 (尺寸不变) [[17, 20, 23], 1, Detect, [nc, anchors]], # 第24层: Detect, 输入3个尺度的特征图 ] ``` #### 3. 配置参数核心含义 **nc**：目标类别数，默认 80（适配 COCO 数据集），自定义数据集需修改为对应类别数。 **depth_multiple/width_multiple**：深度/宽度缩放系数，分别控制模块重复次数和通道数，决定模型规模（S/M/L/X 模型差异核心）。 **anchors**：锚框预设值，3 组特征图各对应 3 组锚框，适配不同尺寸目标检测。 **backbone/head**：网络核心部分，每一行配置格式为 `[from, number, module, args]`，含义如下： from：当前层输入来源，` 1` 表示来自上一层，具体数字表示来自对应层编号（如 6 表示第 6 层输出）。 number：模块重复次数，仅 C3 模块可能大于 1，其他模块默认 1。 module：使用的网络模块（如 Conv、C3、SPPF、Concat、Detect 等）。 args：模块参数（如 Conv 模块参数为 [输出通道、卷积核大小、步长、padding(填充数)]）。 ### 二、用 TensorBoard 可视化模型结构（实操方法） 网络结构可通过 TensorBoard 直观展示，无需手动绘图，同时支持监控训练参数，操作步骤如下： #### 1. 启动 TensorBoard ```Bash # 1. 激活 yolov5 虚拟环境（终端前缀显示 (yolov5)） # 2. 进入 YOLOv5 项目根目录 cd yolov5 master # 3. 启动 TensorBoard，指定日志目录为 runs tensorboard logdir runs ``` #### 2. 查看网络结构 1. 终端输出链接（默认 `http://localhost:6006`），浏览器打开该链接。 2. 点击左侧「Graphs」选项卡，即可看到模型整体结构（输入→DetectionModel→输出）。 3. 交互操作：双击「DetectionModel」展开完整网络；鼠标滚轮缩放、拖拽平移视图；双击模块可查看层级细节。 ### 三、网络结构数据流与特征变换规律 通过 TensorBoard 可视化可清晰梳理数据流，核心规律如下： #### 1. 输入与主干网络特征变换 输入图像尺寸为`1×3×640×640`（batch_size 1，3 通道，分辨率 640×640）。 主干网络通过连续卷积层执行下采样，每次卷积步长为 2 时，特征图尺寸减半，通道数按宽度系数缩放： 第 0 层 Conv：输入 `1×3×640×640` → 输出 `1×32×320×320`（64×0.5 32，尺寸/2）。 依次经过卷积、C3 模块，特征图尺寸逐步缩减为 160×160、80×80、40×40、20×20，通道数逐步增加。 SPPF 模块（第 9 层）为骨干网络最后一层，输出特征图尺寸 `1×1024×20×20`。 #### 2. 头部结构特征融合与检测层 头部结构通过「上采样+拼接」实现多尺度特征融合： 第 10 层 Conv 降维后上采样（20×20→40×40），与第 6 层输出（40×40）拼接，增强中尺度特征。 再次上采样（40×40→80×80），与第 4 层输出（80×80）拼接，增强小尺度特征。 通过卷积下采样恢复尺寸，多次拼接后形成 3 个尺度特征图，用于不同尺寸目标检测。 检测层（Detect）融合 3 个特征图： 第 17 层（80×80，8 倍下采样）：检测小目标。 第 20 层（40×40，16 倍下采样）：检测中目标。 第 23 层（20×20，32 倍下采样）：检测大目标。 ### 四、深度与宽度系数的作用机制 depth_multiple（深度系数）和 width_multiple（宽度系数）是控制模型规模的核心参数，直接影响模块重复次数和通道数： #### 1. 深度系数（depth_multiple） 作用：控制 C3 模块的重复次数，公式为 `实际重复次数 max(round(number×depth_multiple), 1)`。 示例（yolov5s.yaml，depth_multiple 0.33）： backbone 中第 2 层 C3：number 3 → 3×0.33≈1 → 实际重复 1 次。 backbone 中第 4 层 C3：number 6 → 6×0.33≈2 → 实际重复 2 次。 取最大值 1 是为了保证模块至少存在 1 层，避免网络结构缺失。 #### 2. 宽度系数（width_multiple） 作用：控制所有模块的输出通道数，公式为 `实际通道数 round(args[0]×width_multiple)`。 示例（yolov5s.yaml，width_multiple 0.5）： 第 0 层 Conv：args[0] 64 → 64×0.5 32 → 实际输出通道 32。 第 1 层 Conv：args[0] 128 → 128×0.5 64 → 实际输出通道 64。 ### 五、模型构建代码逻辑解析（models/yolo.py） 模型构建核心是解析 YAML 配置并实例化模块，关键代码逻辑从原文提取，步骤如下： #### 1. 配置解析与参数初始化 ```Python # 核心代码逻辑（从原文提取，无修改） def parse_model(d, ch): # d YAML配置字典，ch 输入通道数（初始为3） nc, anchors d.get('nc', 80), d.get('anchors', 3) depth_multiple d.get('depth_multiple', 1.0) width_multiple d.get('width_multiple', 1.0) # 计算输出通道数：锚框数×(类别数+5)，5为坐标(x,y,w,h) + object分数 na (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors no na * (nc + 5) layers, save, c2 [], [], ch[ 1] # layers 网络层列表，save 需保存的层，c2 上一层输出通道数 for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']): m eval(m) if isinstance(m, str) else m # 实例化模块（如Conv、C3） for j, a in enumerate(args): try: args[j] eval(a) if isinstance(a, str) else a # 解析参数 except: pass # 处理深度系数：控制模块重复次数 n n_ max(round(n * depth_multiple), 1) if n > 1 else n if m in [Conv, C3, C3TR, C3Ghost, C3X]: c1, c2 ch[f], args[0] c2 make_divisible(c2 * width_multiple, 8) # 通道数按宽度系数缩放，确保可被8整除 args [c1, c2, *args[1:]] # 更新输入/输出通道数 if m is C3: args.insert(2, n) # 为C3模块插入重复次数参数n n 1 # 重置n，避免重复实例化 # 实例化模块并添加到网络层 m_ nn.Sequential(*[m(*args) for _ in range(n)]) if n > 1 else m(*args) layers.append(m_) save.append(i) if f 1 else None ch.append(c2) return nn.Sequential(*layers), sorted(save) ``` #### 2. 关键步骤说明 模块实例化：通过`eval(m)` 将 YAML 中的模块名称（如 'Conv'）转换为 Python 实例，依赖 `commons.py` 中定义的模块类。 通道数调整：通过 `make_divisible` 确保通道数可被 8 整除，适配 GPU 加速运算。 层保存：标记 `from 1` 的层，供后续特征融合时调用对应层输出。 ### 六、实操关键注意事项 1. 配置文件一致性：修改 `nc`（类别数）后，需确保与数据集类别一致，否则训练报错；锚框需根据自定义数据集尺寸调整，提升检测精度。 2. 模块修改规范：自定义模块（如添加 C4）需先在 `commons.py` 中定义，再在 YAML 配置中调用，且名称需与代码中一致，否则无法实例化。 3. 层数编号意义：网络层从 0 开始编号，头部结构拼接时需准确引用对应层编号（如 [ 1, 6 ] 表示拼接上一层与第 6 层），编号错误会导致特征融合失败。 4. 可视化依赖：TensorBoard 需在训练后生成日志文件（存于 runs 目录），若未训练可先运行 `python train.py` 生成日志，再启动可视化。 ### 七、整体流程回顾 1. 结构定义：在 `models/yolov5s.yaml` 中配置 backbone、head、参数系数及锚框。 2. 可视化验证：通过 TensorBoard 查看网络结构，确认数据流与特征变换规律。 3. 模型构建：`yolo.py` 解析 YAML 配置，按系数调整模块重复次数与通道数，实例化网络。 4. 自定义优化：修改配置文件（如调整系数、替换模块），或在 `commons.py` 中添加新模块，实现网络结构创新。 核心原则：理解模型构建原理是结构修改与优化的前提，需结合配置文件、代码逻辑与可视化结果，确保每一处修改都符合网络数据流规律。"},"/note/机器人项目笔记/YOLO/4.1-YOLOv5 Torchhub 模型预测进阶.html":{"title":"YOLOv5 Torchhub 模型预测进阶","content":"# YOLOv5 Torchhub 模型预测进阶 ### 一、核心基础：模型加载 #### 1. 加载核心代码 ```Python model torch.hub.load(\"./\",\"custom\", path \"runs/train/exp2/weights/best.pt\", source \"local\") ``` #### 2. 加载参数详解 `\"./\"`：模型所在路径，必须是 YOLOv5 项目根目录（需包含 `hubconf.py` 文件，否则报错）。 `\"custom\"`：固定关键字，用于加载自定义训练模型，不可替换为其他值。 `path \"runs/train/exp2/weights/best.pt\"`：模型文件路径（支持 `.pt` 原生模型、`.engine` TensorRT 模型）。 `source \"local\"`：加载来源，仅支持 `github` 或 `local` 两种类型，本地模型必须设为 `local`。 #### 3. 加载流程与依赖 底层逻辑：通过 `hubconf.py` 中的 `custom()` 函数创建模型，自动处理 `auto_shape` 封装，支持多格式输入。 关键依赖：必须在包含 `hubconf.py` 的 YOLOv5 根目录运行代码，否则会因找不到配置文件加载失败。 官方模型加载：若加载官方预训练模型（如 `yolov5n.pt`、`yolov5m.pt`），可直接指定模型名称，无需 `path` 参数（需联网下载）。 ### 二、灵活预测：支持多格式输入 #### 1. 预测核心代码 ```Python # 模型预测（支持多种输入格式） results model(image) ``` #### 2. 支持的输入类型（无需额外预处理） 文件类：本地图片路径（如 `\"./test.jpg\"`）、视频路径（如 `\"./test.mp4\"`）。 网络类：在线图片 URI（如 `\"https://xxx.jpg\"`）。 数据类：OpenCV 读取的 `numpy` 数组、PIL 图片对象、PyTorch 张量。 ### 三、结果解析：5 种常用结果处理方法 所有结果均基于 `results` 对象（YOLOv5 封装的 `Detections` 类），支持以下方法，代码从原文提取： #### 1. 图片可视化（直接显示检测结果） ```Python # 弹出窗口显示带标注的图片 results.show() ``` #### 2. 图片结果（获取标注后的图像数据） ```Python # 返回标注后的图像张量（可直接保存或转换为 PIL/OpenCV 格式） annotated_img results.render()[0] ``` #### 3. 数据结果（获取检测坐标与类别信息） ```Python # 返回 DataFrame 格式数据，支持 4 种坐标类型 # 坐标类型：xyxy（左上 右下坐标）、xyxyn（归一化 xyxy）、xywh（中心 宽高）、xywhn（归一化 xywh） data results.pandas().xyxy[0] # 推荐使用 xyxy 格式，直观易懂 print(data[[\"xmin\", \"ymin\", \"xmax\", \"ymax\", \"name\", \"confidence\"]]) ``` #### 4. 裁切结果（单独提取检测目标） ```Python # 返回列表，每个元素为字典（包含 box 坐标、confidence 置信度、class 类别、im 裁切图像） crops results.crops # 示例：保存第一个检测目标的裁切图 crops[0][\"im\"].save(\"crop_0.jpg\") ``` #### 5. 文本结果（打印检测信息） ```Python # 终端打印检测结果（类别、置信度、坐标） results.print() # 提取文本字符串（用于日志或界面显示） text_result str(results) ``` ### 四、特殊适配：TensorRT 模型加载与问题解决 #### 1. TensorRT 模型加载代码 ```Python # 加载 TensorRT 模型（.engine 格式） model torch.hub.load(\"./\", \"custom\", path \"yolov5s.engine\", source \"local\") ``` #### 2. 两大关键问题解决 ##### （1）输入维度不匹配报错 问题：Torchhub 不会自动调整图像尺寸为 640×640（`detect.py` 会自动调整），可能导致维度不匹配。 解决方案：加载与输入尺寸匹配的 TensorRT 模型（如 FP16/FP32 格式，确保模型输入维度与图像维度一致）。 ##### （2）类别名称显示异常（class 0、class 27 等） 问题：TensorRT 模型默认未加载类别名称，仅显示类别索引。 解决方案：手动指定类别名称映射，代码如下（从原文提取）： ```Python # 手动设置类别名称（以 COCO 数据集为例，按需修改） model.names {0: \"person\", 27: \"tie\", ...} # 索引对应类别名称 ```"},"/note/机器人项目笔记/YOLO/1.1-YOLOv5 环境安装.html":{"title":"YOLOv5 Windows 11 环境安装","content":"# YOLOv5 Windows 11 环境安装 ## 一、安装前准备 ### 1. 系统与硬件要求 系统：Windows 11（课程基于该系统演示，Win10 可参考适配） CPU：无强制要求（示例为 i5 8300H） 显卡：NVIDIA 独立显卡（示例为 1050Ti，需匹配对应 CUDA 版本） 显卡驱动：需支持目标 PyTorch 版本（示例驱动版本 516.94） 网络：稳定网络（用于下载安装包与依赖，部分包体积较大） ### 2. 核心下载地址 Miniconda（清华镜像）：用于创建隔离 Python 环境 PyPI 国内源（清华大学）：加速 Python 包下载 PyTorch 官网：获取指定版本安装命令 YOLOv5 GitHub 仓库：下载 7.0 稳定版源码 ## 二、分步安装流程 ### 第一步：安装 Miniconda（轻量级环境管理器） #### 目的 隔离不同项目的 Python 环境，避免依赖冲突，比 Anaconda 更轻量化（无需预制多余包）。 #### 操作步骤 1. 下载版本：访问Miniconda清华镜像具体地址 `https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/`，搜索 `PY38`，选择 `Miniconda3 py38_22.11.1 1 Windows x86_64.exe`（Windows 64 位，Python 3.8 兼容版），其他系统需匹配对应版本。 2. 安装向导： 双击安装包，点击「Next」→ 勾选「I agree」→ 选择安装类型「Just Me」（个人使用）。 安装路径：优先选 C 盘（无空间可选其他盘），**路径必须纯英文无空格**（避免安装失败）。 关键勾选：务必勾选「Add Miniconda3 to my PATH environment variable」（虽系统提示不推荐，但后续使用必需）。 完成安装：点击「Install」→ 后续步骤取消两个默认勾选 → 点击「Finish」。 ### 第二步：创建并激活 Python 虚拟环境 #### 核心命令 1. 打开 CMD：按 Win 键 + R，输入 `CMD` 打开命令提示符。 2. 创建环境： ```Bash conda create n yolov5 python 3.8 ``` 说明：`yolov5` 为环境名称（可自定义），`python 3.8` 为指定 Python 版本（必需，避免高版本兼容问题）。 3. 确认创建：输入 `y` 回车，等待环境创建完成。 4. 激活环境： ```Bash conda activate yolov5 ``` 验证：命令行前缀显示 `(yolov5)` 即激活成功。 ### 第三步：配置 PyPI 国内源（加速包下载） #### 目的 解决 Python 官方仓库（国外）下载慢的问题，使用清华大学镜像。 #### 配置命令 在激活的 `yolov5` 环境中执行： ```Bash pip config set global.index url https://pypi.tuna.tsinghua.edu.cn/simple ``` 说明：命令会将国内源写入 `pip.ini` 文件，全局生效，后续 `pip install` 自动走国内源。 ### 第四步：安装 PyTorch 1.8.2（YOLOv5 兼容版） #### 版本选择理由 当前 PyTorch 已更新至 2.1.20，但 YOLOv5 7.0 对高版本兼容性不佳，**必须安装 1.8.2 版本**，避免训练时出现报错。 #### 按显卡型号选择 CUDA 版本 显卡类型推荐 CUDA 版本原因 30 系（3050/3060 等）、40 系11.1不支持 CUDA 11 以下版本 16 系（1650/1660 等）10.2半精度训练兼容性要求 无显卡/AMD 显卡CPU 版本无需 CUDA，直接安装 CPU 版 PyTorch 老显卡（如 1050Ti）11.1兼容性更优 #### 安装命令 1. 复制对应版本命令（从 PyTorch 官网「Previous Versions」获取）： CUDA 11.1 版本（30/40 系、老显卡）： ```Bash pip install torch 1.8.2+cu111 torchvision 0.9.2+cu111 torchaudio 0.8.2 f https://download.pytorch.org/whl/lts/1.8/torch_lts.html ``` CUDA 10.2 版本（16 系）： ```Bash pip install torch 1.8.2+cu102 torchvision 0.9.2+cu102 torchaudio 0.8.2 f https://download.pytorch.org/whl/lts/1.8/torch_lts.html ``` CPU 版本（无 NVIDIA 显卡）： ```Bash pip install torch 1.8.2+cpu torchvision 0.9.2+cpu torchaudio 0.8.2 f https://download.pytorch.org/whl/lts/1.8/torch_lts.html ``` 2. 执行命令：在激活的 `yolov5` 环境中粘贴命令，回车安装。 注意：安装包约 3.1G，首次下载可能较慢，需耐心等待；若之前安装过，会优先使用缓存加速。 #### 验证 GPU 可用性（关键步骤） 无需额外安装 CUDA 和 CUDNN，仅需 PyTorch 安装正确 + 显卡驱动兼容： 1. 在 CMD 中执行： ```Bash python import torch torch.cuda.is_available() # 返回 True 则说明 GPU 可调用 ``` 2. 查看显卡驱动：桌面 → 右键 → 英伟达控制面板 → 系统信息 → 组件 → 查看 `NV CUDA 64.DLL` 版本（即驱动支持的最高 CUDA 版本，需 ≥ 安装的 CUDA 版本）。 若驱动不兼容：升级显卡驱动后重试。 ### 第五步：下载 YOLOv5 7.0 源码并配置依赖 #### 1. 下载源码 访问 YOLOv5 GitHub 仓库 → 点击「Releases」→ 选择「V7.0」→ 下载「Source code (zip)」。 解压：将 zip 文件解压到桌面，确保桌面生成 `yolov5 7.0` 文件夹（路径纯英文无空格）。 #### 2. 修改 requirements.txt（适配版本） 打开 `yolov5 7.0` 文件夹中的 `requirements.txt`，调整 3 处配置： `numpy 1.20.3`（指定版本，避免兼容问题） `pillow 8.3.0`（指定版本，适配 YOLOv5 7.0） 注释掉 `torch` 和 `torchvision` 相关行（已手动安装，避免重复安装为 CPU 版本）： ```Plain Text # torch> 1.7.0 # torchvision> 0.8.1 ``` #### 3. 安装 YOLOv5 依赖 1. 进入源码目录： 方法 1：CMD 中执行 `cd Desktop\\yolov5 7.0`（根据实际解压路径调整）。 方法 2：打开 `yolov5 7.0` 文件夹，地址栏输入 `CMD` 回车（直接进入当前目录的 CMD）。 2. 激活环境（若已退出）：`conda activate yolov5`。 3. 安装依赖： ```Bash pip install r requirements.txt ``` 验证：无报错且显示「Successfully installed」即依赖安装完成。 ### 第六步：模型检测（验证环境是否生效） #### 1. 下载预训练模型 首次运行 `detect.py` 时，系统会自动下载 `yolov5s.pt` 预训练模型（约 140M）。 加速方案：若终端下载慢，复制终端中显示的模型下载链接，用浏览器下载，下载后复制到 `yolov5 7.0` 根目录。 #### 2. 运行检测脚本 在 `yolov5 7.0` 目录的 CMD 中执行： ```Bash python detect.py ``` #### 3. 验证结果 若终端显示 `GPU:0` 或 `CUDA:` 相关信息，说明 GPU 版本生效。 若显示 `CPU`，说明环境配置失败（需检查 PyTorch 安装版本与显卡适配性）。 检测结果：生成 `runs/detect/exp2` 文件夹，内含测试图片的检测结果（框选目标），即环境搭建成功。 ## 三、关键注意事项 1. 路径规范：所有安装路径、文件夹名称必须纯英文无空格，否则会导致脚本运行失败。 2. 版本匹配：Python 3.8 + PyTorch 1.8.2 + YOLOv5 7.0 为稳定组合，不可随意升级版本。 3. 无需额外安装 CUDA/CUDNN：PyTorch 已内置对应版本，仅需显卡驱动支持即可调用 GPU。 4. 常见问题： 环境激活失败：检查 Miniconda 是否添加到系统 PATH。 依赖安装报错：删除 `requirements.txt` 中冲突的包版本，重新执行 `pip install`。 GPU 调用失败：确认显卡型号、CUDA 版本、驱动版本三者匹配，重新安装 PyTorch。"},"/note/机器人项目笔记/YOLO/1.6-YOLOv5 Gradio搭建Web GUI.html":{"title":"YOLOv5 Gradio搭建Web GUI实战","content":"# YOLOv5 Gradio搭建Web GUI实战 ### 一、Gradio 简介与环境安装 #### 1. Gradio 核心优势 Gradio 是开源 Python 库，专为机器学习演示和 Web 应用开发设计，内置丰富交互组件，已封装前后端交互逻辑，无需额外编写复杂代码，仅需少量代码即可实现可视化 Web 界面。 #### 2. 环境安装（基于 yolov5 虚拟环境） 1. 打开终端，激活前文创建的 yolov5 虚拟环境： ```Bash conda activate yolov5 ``` 2. 执行安装命令，下载并安装 Gradio： ```Bash pip install gradio ``` 3. 验证安装：安装完成后无报错，即可开始后续开发（国内源可加速下载，无需额外配置）。 ### 二、完整 Web GUI 实现（集成全拓展功能） 以下为集成所有核心及拓展功能的完整代码，已通过注释标注关键功能点（实时更新、示例图片、公网分享等）。该代码一次性实现双输入源、参数调节、实时检测、示例预设、公网分享五大功能. ```Python import torch import gradio as gr model torch.hub.load(\"./\",\"custom\", path \"runs/train/exp/weights/best.pt\", source \"local\") title \"YOLOv5 Object Detection\" description \"Upload an image to detect objects using a custom trained YOLOv5 model.\" base_conf 0.25 base_iou 0.45 def det_image(img,conf,iou): model.conf conf model.iou iou results model(img) return results.render()[0] gr.Interface(inputs [gr.Image(sources [\"webcam\", \"upload\"], label \"选择输入源\", type \"numpy\"),gr.Slider(minimum 0.0,maximum 1.0,value base_conf),gr.Slider(minimum 0.0,maximum 1.0,value base_iou)], outputs [\"image\"], #1.Gradio的设计哲学是\"声明式\"编程：你声明输入组件、函数和输出组件，Gradio负责连接它们 \t\t\t #2.fn 参数指定的函数是Gradio自动调用的回调函数 \t\t\t #3.参数传递顺序完全由inputs列表的顺序决定 fn det_image, title title, description description, #实时更新 live True, #示例图片 examples [[\"./datasets/images/train/90.jpg\",base_conf,base_iou],[\"./datasets/images/train/120.jpg\",0.35,base_iou]]).launch(share True)#公网 ``` #### 代码结构与功能对应解析 代码按“模型加载→参数定义→检测函数→界面配置”逻辑组织，每处拓展功能均与注释精准对应，具体说明如下： 1. **模型加载**：通过`torch.hub.load` 加载本地训练好的模型，路径为 `runs/train/exp/weights/best.pt`，`source \"local\"` 指定从本地加载，避免网络请求，适配自定义训练模型。 2. **核心参数**：默认置信度阈值 `base_conf 0.25`、IOU 阈值 `base_iou 0.45`，与 YOLOv5 原生默认参数一致，确保检测效果基础稳定。 3. **检测函数**：`det_image` 函数接收图片、置信度、IOU 三个参数，动态更新模型阈值后执行检测，返回渲染后的检测结果（含目标标注框）。 4. **界面配置（拓展功能对应）**： 双输入源：`gr.Image(sources [\"webcam\", \"upload\"])` 支持摄像头拍摄和本地图片上传两种方式，`type \"numpy\"` 适配模型输入格式，标签设为“选择输入源”，操作直观。 5. 参数调节：两个 `gr.Slider` 组件分别控制置信度、IOU 阈值，范围限定为 0.0 1.0，默认值关联前文定义的基础参数，支持实时微调检测精度。 6. 实时更新（注释对应）：`live True` 开启动态触发机制，上传图片、调节滑动条或摄像头拍摄后，右侧结果将自动刷新，无需手动点击提交。 7. 示例图片（注释对应）：`examples` 预设两组示例数据，分别为 `./datasets/images/train/90.jpg`（默认阈值）和 `./datasets/images/train/120.jpg`（置信度阈值 0.35），用户可直接点击示例快速测试。 8. 公网分享（注释对应）：`launch(share True)` 生成临时公网链接，支持异地访问，默认有效期 72 小时，适合演示和团队协作测试。 #### 运行与测试步骤 1. 保存代码：将上述代码保存为 `yolov5_gradio.py`，放置于 YOLOv5 根目录，确保与 `runs`、`datasets` 文件夹同级。 2. 启动程序：在激活的 yolov5 虚拟环境中，执行命令 `python yolov5_gradio.py`。 3. 访问界面：终端将输出两个地址，本地访问地址 `http://localhost:7860`（Ctrl+点击直接打开）、公网访问地址 `https://xxx.gradio.live`（72 小时有效）。 4. 功能测试： 上传检测：点击“选择输入源”上传 `datasets/images/train` 下的图片，右侧自动显示标注结果，调节滑动条可过滤冗余框或补充漏检目标。 5. 摄像头检测：点击输入框下方“摄像头”按钮，授权后可实时拍摄并检测，适合动态场景测试。 6. 示例测试：直接点击界面下方预设示例，快速验证模型检测效果，无需手动上传。 ### 三、整体流程回顾与优势总结 流程梳理：环境安装（Gradio）→ 编写完整代码（集成全功能）→ 运行测试 → 公网分享，全程无需编写前后端分离代码，仅通过 Gradio 组件配置即可实现 Web 可视化。 #### 运行与测试 1. 执行代码，终端将提示本地访问地址：`http://localhost:7860`。 2. 按 `Ctrl+点击` 地址打开浏览器，左侧上传图片（如死神VS火影数据集图片），右侧将自动显示检测结果。"},"/note/机器人项目笔记/YOLO/3.4-YOLOv5 替换主干网络（以 MobileNetV3 为例).html":{"title":"YOLOv5 替换主干网络（以 MobileNetV3 为例）","content":"# YOLOv5 替换主干网络（以 MobileNetV3 为例） ### 一、前置核心原则与准备工作 #### 1. 替换核心原则 YOLOv5 的 head 部分依赖 backbone 输出的 3 个关键尺度特征图，替换主干网络后需满足： 必须保留 **8倍下采样（80×80）**、**16倍下采样（40×40）**、**32倍下采样（20×20）** 的特征输出。 仅替换 backbone，head 部分可复用（只需调整 `from` 参数指向新的特征层），无需重构。 #### 2. 代码借鉴与依赖准备 ##### （1）代码借鉴来源 ```Plain Text # MobileNet 代码来源：PyTorch 内置 torchvision 库 # 无需额外下载，直接通过 torchvision.models 调用 import torchvision.models as models ``` ##### （2）依赖安装 ```Bash # 安装模型结构可视化工具（查看 MobileNet 特征图输出） pip install torchinfo ``` ##### （3）MobileNet 结构分析（关键步骤） 通过 `torchinfo` 可视化 MobileNetV3 Small 结构，确认其特征图下采样与通道数： ```Python # 从原文提取的结构分析代码（无任何修改） import torch import torchvision.models as models from torchinfo import summary # 加载 MobileNetV3 Small 预训练模型 model models.mobilenet_v3_small(pretrained True, progress True) # 可视化结构（输入尺寸：1×3×640×640，与 YOLOv5 一致） summary(model, input_size (1, 3, 640, 640)) ``` 关键发现：MobileNetV3 Small 的特征提取层（`model.features`）天然包含 3 个关键尺度： 8倍下采样（80×80）：前 4 个模块输出（通道数 24）。 16倍下采样（40×40）：第 4 9 个模块输出（通道数 48）。 32倍下采样（20×20）：第 9 个模块后输出（通道数 576）。 ### 二、替换主干网络全流程（4步完成） 核心修改顺序： `models/commons.py` → 加入 MobileNet 模块 → `models/yolov5*.yaml` → 替换 backbone+调整 `from` 参数 → `models/yolo.py` → 配置传参细节 → `train.py` → 训练时指定配置文件 #### 第一步：修改 models/[commons.py](commons.py)（定义 MobileNet 模块） 1. 打开 `models/commons.py`，添加 MobileNetV3 Small 模块（利用切片复用预训练特征提取层）： ```Python # 从原文提取的 MobileNetV3 模块（无修改） import torch import torch.nn as nn import torchvision.models as models class MobileNetV3(nn.Module): def __init__(self, slice): super(MobileNetV3, self).__init__() # 加载预训练 MobileNetV3 Small self.model None # 根据 slice 参数选择特征提取层片段 if slice 1: # 片段1：前4个模块，输出 8倍下采样（80×80，通道24） self.features self.model.features[:4] elif slice 2: # 片段2：第4 9个模块，输出 16倍下采样（40×40，通道48） self.features self.model.features[4:9] else: # 片段3：第9个模块后，输出 32倍下采样（20×20，通道576） self.features self.model.features[9:] def forward(self, x): return self.features(x) ``` #### 第二步：修改 models/yolov5*.yaml（替换 backbone+调整层编号） 1. 复制 `models/yolov5s.yaml`，重命名为 `yolov5s_mobilenet.yaml`。 2. 替换 backbone 部分（删除原有 C3/Conv 结构，添加 MobileNet 片段）： ```YAML # 修改后的 backbone 配置 nc: 80 depth_multiple: 0.33 width_multiple: 0.50 anchors: [10,13, 16,30, 33,23] [30,61, 62,45, 59,119] [116,90, 156,198, 373,326] # 替换后的 backbone：3个 MobileNet 片段，对应 3个关键尺度 backbone: [[ 1, 1, MobileNetV3, [24,1]], # 片段0：输出 80×80（8倍下采样），通道24 [ 1, 1, MobileNetV3, [48,2]], # 片段1：输出 40×40（16倍下采样），通道48 [ 1, 1, MobileNetV3, [576,3]], # 片段2：输出 20×20（32倍下采样），通道576 ] # head 部分仅调整 from 参数（适配新 backbone 层编号） head: [[ 1, 1, Conv, [512, 1, 1]], #3 20*20 [ 1, 1, nn.Upsample, [None, 2, 'nearest']], #4 40*40 [[ 1, 1], 1, Concat, [1]], # cat backbone p1, 5 40*40 [ 1, 3, C3, [512, False]], #6 40*40 [ 1, 1, Conv, [256, 1, 1]], #7 40*40 [ 1, 1, nn.Upsample, [None, 2, 'nearest']], #8 80*80 [[ 1, 0], 1, Concat, [1]], # cat backbone P0, 9 80*80 [ 1, 3, C3, [256, False]], #10 80*80 [ 1, 1, Conv, [256, 3, 2]], #11 40*40 [[ 1, 7], 1, Concat, [1]], # cat head P7 12 40*40 [ 1, 3, C3, [512, False]], #13 40*40 [ 1, 1, Conv, [512, 3, 2]], #14 20*20 [[ 1, 3], 1, Concat, [1]], # cat head P3 15 20*20 [ 1, 3, C3, [1024, False]], #16 20*20 [[10, 13, 16], 1, Detect, [nc, anchors]], # Detect(P3, P4, P5) ] ``` 1. 关键调整：head 中所有 Concat`from` 参数需根据新 backbone 层编号重新映射。 #### 第三步：修改 models/[yolo.py](yolo.py)（配置 MobileNet 传参细节） 1. 打开 `models/yolo.py`，找到 `parse_model` 函数，新增 MobileNetV3 的传参逻辑： ```Python elif m is MobileNetV3: c2 args[0] args args[1:] ``` #### 第四步：修改 [train.py](train.py)（指定新配置文件） 1. 打开 `train.py`，修改 ` cfg` 参数默认值，指向 MobileNet 配置文件： ```Python # 从原文提取的修改后代码（无任何更改） parser.add_argument(' cfg', type str, default ROOT / 'models/yolov5s_mobilenet.yaml', help 'model.yaml path') ```"},"/note/机器人项目笔记/YOLO/YOLO物体检测原理与应用.html":{"title":"YOLO物体检测原理与应用","content":"# YOLO物体检测原理与应用 **核心定位**：端到端实时物体检测算法，将检测任务转化为回归问题，快速实现“找物体+画框” ## 一、核心思想 1. **核心目标**：从图像像素直接回归出**边界框坐标**和**类别概率**，端到端检测 2. **核心流程**（四步口诀：划网 预测 筛选 去重） 输入图片 → 划分S×S网格 → 网格预测物体信息 → 非极大值抑制（NMS）筛选最终框 3. **关键规则**：物体由**中心点所在网格**负责预测 ## 二、基本原理（以早期版本为例） ### 1. 输入输出格式 类型 内容 说明 输入 图片+标签 标签包含：置信度`Pc`、框坐标`Bx/By/Bh/Bw`、类别概率（one hot） 输出 网格预测向量 例：8维向量`[Pc,Bx,By,Bh,Bw,C1,C2,C3]` ### 2. 核心操作 **网格划分**：S×S网格（常用19×19），每个网格预测固定长度向量 **置信度** **`Pc`**：表示网格内有物体的概率（1 有，0 无） **坐标归一化**：`Bx/By`是相对于网格的位置（0 1），`Bh/Bw`是相对于整图的比例 ### 3. 检测两步走 1. **置信度筛选**：过滤`Pc < 阈值`（如0.4）的预测框 2. **非极大值抑制（NMS）** 目标：解决同一物体的重复框问题 步骤：选置信度最高框 → 计算其他框与它的IoU → 删除`IoU>0.5`的框 → 重复至结束 ## 三、YOLO版本演进（核心改进+适用场景） 版本 核心改进 适用场景 V1 单网格单框，端到端基础架构 入门理解原理 V2 引入锚框（Anchor Boxes），单网格多框 检测不同形状物体 V3 多尺度检测（浅层小物体，深层大物体） 大中小目标兼顾检测 V4 Mosaic增强、DropBlock、注意力机制 提升检测精度，复杂场景 V7 统一3×3卷积核，简化网络 追求推理速度，嵌入式部署 V8 官方维护，支持检测/分割/姿态估计，代码友好 项目开发、研究基线（**首选**） V9 优化梯度传播，增强浅层特征学习 深层网络精度提升研究 ## 四、数据集准备 ### 1. 数据集来源 现成平台：Roboflow（规范YOLO格式）、Kaggle 自制：实拍/爬虫 → LabelImg标注 → 半自动标注（少量标注→训练初级模型→伪标签修正） ### 2. 标准结构 ```Plain Text dataset/ ├── images/ （训练/验证图片） └── labels/ （对应标签文件） ``` ### 3. 配置文件（data.yaml） ```YAML train: 训练集路径 val: 验证集路径 nc: 类别数 names: [类别1, 类别2,...] ```"},"/note/机器人项目笔记/YOLO/1.4-YOLOv5 模型训练.html":{"title":"YOLOv5 模型训练","content":"# YOLOv5 模型训练 ## 一、训练前数据集格式整理（必做步骤） 标注完成后需按 YOLOv5 标准格式整理数据集，确保模型能正确读取图片与标签文件，这是训练成功的基础前提。 ### 1. 标准文件夹结构 需创建顶层 `images` 和 `labels` 文件夹，且名称不可修改，内部均划分 `train`（训练集）和 `val`（验证集）子文件夹，结构如下： ```plain text dataset/ ├─ images/ # 存放所有图片 │ ├─ train/ # 训练集图片 │ └─ val/ # 验证集图片 └─ labels/ # 存放所有标注文件（与图片一一对应） ├─ train/ # 训练集标签文件 └─ val/ # 验证集标签文件 ``` ### 2. 实操整理步骤 1. 数据集划分：因标注图片仅41张，随机抽取部分作为验证集，具体为将编号300至480的7张图片移入 `images/val`，其余图片全部放入 `images/train`。 2. 标签文件同步：在 `labels` 文件夹下创建 `train` 和 `val` 子文件夹，将300至480对应的7个标注文件移入 `labels/val`，剩余标注文件放入 `labels/train`，确保图片与标签文件名完全一致。 3. 辅助文件调整：将自动生成的 `classes.txt`和视频文件 移至images同层文件夹,文件夹名称、文件对应关系必须严格遵循上述规范，否则模型将无法读取数据，导致训练报错。 关键提醒：文件夹名称、文件对应关系必须严格遵循上述规范，否则模型将无法读取数据，导致训练报错。 ## 二、核心训练参数配置 YOLOv5 训练仅需重点配置两个核心参数：`weights`（预训练权重）和 `data`（数据集配置文件），参数设置正确即可启动训练。 ### 1. 核心参数说明 参数名称作用配置要求 `weights`指定预训练权重文件，基于官方模型微调，无需从头训练与前文检测模块使用的权重文件格式一致（如 `yolov5s.pt`），放置于 YOLOv5 根目录 `data`指定数据集描述文件（.yaml格式），定义数据路径、类别映射复制官方 `coco128.yaml` 并修改，适配自定义数据集 ### 2. 数据集配置文件（.yaml）修改 #### 步骤1：复制配置文件 进入 YOLOv5 根目录 `data` 文件夹，复制 `coco128.yaml` 并重命名为 `bvn.` `yaml`。 #### 步骤2：修改文件内容 ```yaml # 数据集配置文件 bvn.yaml path: ./dataset # 数据集根目录（相对YOLOv5 master项目路径） train: images/train # 训练集图片路径（相对于path） val: images/val # 验证集图片路径（相对于path） test: # 测试集（可选，本次不配置） # 类别映射（仅保留daitu、mingren两类） names: 0: mingren 1: daitu ``` 说明：路径采用相对路径，无需追溯上级目录，确保 `dataset` 文件夹已放入 YOLOv5 根目录。 ### 3. 参数指定方式 1. 命令行指定（临时生效）：训练时在命令中直接携带参数，示例如下： `python train.py weights yolov5s.pt data data/` `bvn` `.yaml` 2. 配置文件修改（永久生效）：直接编辑 `train.py`，将默认参数 ` data` 对应的路径改为 `data/` `bvn` `.yaml`，无需每次命令行输入。 ## 三、模型训练启动与过程监控 ### 1. 启动训练 在激活的 yolov5 虚拟环境中，进入 YOLOv5 根目录，执行上述训练命令，系统将自动完成以下操作： 打印参数配置、系统环境、模型结构、优化器等信息； 提示可通过 TensorBoard 查看训练日志； 扫描训练集、验证集数据，生成缓存文件，随后启动训练。 ### 2. TensorBoard 日志监控 训练过程中可通过 TensorBoard 实时查看指标变化，操作步骤如下： ```Bash # 启动TensorBoard，指定日志目录（runs/train） tensorboard logdir runs/train ``` 启动后，终端将显示访问地址 `localhost:6006`，按住 Ctrl 点击地址，浏览器打开后可查看： 训练/验证集损失变化（box loss、classes loss、object loss）； 各类评估指标趋势（MAP50 等）； 模型结构可视化（graphs）； 多轮训练结果对比（如 EXP2、EXP3 等）。 终止 TensorBoard：命令行按 `Ctrl+C` 即可退出。 ## 四、训练常见问题及解决方案 ### 1. Arial.ttf 字体文件无法下载（卡顿） 问题现象：训练时长时间卡在字体下载步骤，进度缓慢； 解决方案：手动下载 Arial.ttf 字体，放置于 Windows 对应目录： `C:/Users/用户名/AppData/Roaming/Ulralytics` 说明：训练时终端会提示具体下载路径，按路径放置字体即可。 ### 2. 页面文件太小，无法完成操作 问题原因：内存不足，或环境安装在非系统盘（如D盘）导致虚拟内存不足； 解决方案： 1. 调整训练参数 `workers 1`（减少内存占用）； 2. 修改虚拟内存： 打开「设置」→「系统」→「关于」→「高级系统设置」； 点击「性能」→「设置」→「高级」→「虚拟内存」； 取消「自动管理所有驱动器的分页文件大小」，选中环境安装所在盘（如D盘）； 设置自定义大小（50000~100000 MB），点击「设置」→「确定」，重启电脑生效。 ### 3. Upsample object has no attribute 'recompute_scale_factor' 问题原因：PyTorch 版本过高，与 YOLOv5 不兼容； 解决方案（二选一）： 1. 降低 PyTorch 版本至 1.8.2（推荐，无报错稳定版本，前文环境已适配）； 2. 修改 PyTorch 源码：打开报错的 `upsampling.py` 文件，删除对应行的 `recompute_scale_factor` 参数，忽略编辑器危险提示，保存即可。 ## 五、训练结果分析与文件说明 ### 1. 训练结果核心指标 本次实操训练 100 轮后，核心指标如下（供参考）： 整体指标：MAP50 0.936（模型整体检测精度优秀）； 单项指标：mingren 类别精度 0.96，daitu 类别精度略低，存在类别检测差异。 ### 2. 训练结果文件（保存于 runs/train/EXP* 目录） 文件/文件夹作用 `weights/best.pt`训练过程中性能最优的模型权重（优先选用） `weights/last.pt`最后一轮训练的模型权重（备用） `events.out.tfevents*`TensorBoard 日志文件，用于复盘训练过程 `results.csv`记录每一轮训练的各项指标，支持自定义绘图分析 `results.png`自动生成的指标整合图，直观展示各项指标变化趋势 `labels/`数据集类别分布、比例等统计信息 ## 六、训练后模型检测实操 使用训练得到的最优权重 `best.pt` 对视频进行检测，验证模型效果，命令完全复刻实操代码： ```Bash python detect.py weights runs/train/EXP2/weights/best.pt source ./datasets/BVN.mp4 view img ``` ### 参数说明 ` weights`：指定训练生成的最优权重文件路径； ` source`：指定检测目标（本次为数据集下的 BVN.mp4 视频）； ` view img`：实时可视化检测结果，弹窗显示每帧检测效果。 ### 检测结果与终止方式 效果：简单场景下检测准确，动态特效较多时精度略有下降，整体表现良好； 终止检测：不可直接关闭弹窗，需在命令行按 `Ctrl+C` 终止程序。"},"/note/机器人项目笔记/YOLO/2.1-YOLOv5 使用 AutoDL 服务器训练.html":{"title":"YOLOv5 使用 AutoDL 服务器训练","content":"# YOLOv5 使用 AutoDL 服务器训练 ## 一、AutoDL 服务器基础操作 ### 1. 平台访问与登录 1. 访问地址：百度搜索「AutoDL」，进入官方网站（`www.autodl.com`），完成注册并登录（注册流程略，需自行充值余额以租用算力）。 2. 登录后进入控制台，默认显示「机器实例」页面，初始无运行实例，需先租用算力。 ### 2. 算力租用（GPU 选择） #### 核心选择逻辑 进入「算力市场」，按需求选择显卡型号、租赁方式及地区，关键要点如下： 租赁方式：支持按量计费、包日、包周、包月，演示场景推荐「按量计费」，灵活控制成本。 地区选择：无特殊要求，优先选择有可用卡、延迟较低的地区即可。 显卡型号：根据数据量和需求选择，演示场景可选 RTX 3080、RTX 2080Ti；大批次训练可选择 V100、RTX 3090，多卡训练需在「GPU」选项中选择对应数量（如选 2 即支持双卡训练）。 #### 租用实操步骤 1. 在算力列表中筛选可用显卡（标注「1卡可租」「网盘支持」），选择 RTX 2080Ti 或 3080，点击进入配置页。 2. 数据盘配置：默认提供 50GB 数据盘，数据量不大时无需扩容；若数据集过大，需确认机器支持后再扩容。 3. 镜像选择（关键坑点）： ❌ 禁止选择 YOLOv5 算法镜像：虽可运行，但训练结果分数会为 0，完全无效。 ✅ 选择基础镜像：选「PyTorch」镜像，版本推荐 PyTorch 1.9+、Python 3.8、CUDA 11.1；若租用 30 系列显卡，CUDA 版本必须 ≥11.0，否则不兼容。 4. 点击「立即创建」，等待服务器启动（约 1 2 分钟），启动后状态显示「运行中」。 ### 3. 服务器连接（Jupyter Lab） 服务器启动后，点击实例后的「Jupyter Lab」，自动在浏览器中打开交互界面，左侧为文件列表，右侧可创建代码块或启动终端，用于后续环境配置和训练操作。 ## 二、服务器环境配置（YOLOv5 部署） ### 1. 项目代码与文件上传 1. 上传 YOLOv5 代码：将本地下载的 YOLOv5 master 分支代码（压缩包或文件夹），通过 Jupyter Lab 左侧「上传文件」按钮拖拽上传，或点击「Upload」选择文件上传。 2. 上传预训练模型：将 `yolov5s.pt` 模型文件上传至服务器（与 YOLOv5 代码同级目录），用于后续检测验证。 3. 上传数据集：将数据集压缩为 ZIP 文件（清理无关内容，如 `coco128` 文件夹），上传至服务器后解压使用。 ### 2. 代码解压与环境激活 在 Jupyter Lab 中启动终端，执行以下命令（代码完全复刻实操）： ```Bash # 解压 YOLOv5 代码压缩包（文件名按实际上传的修改） unzip yolov5 master.zip # 进入项目目录 cd yolov5 master # 初始化环境（解决镜像环境未激活问题） conda init # 退出终端并重新启动（重启后环境自动激活） exit ``` 重新启动终端后，再次进入 `yolov5 master` 目录，执行依赖安装命令。 ### 3. 依赖包安装 ```Bash # 安装项目依赖 pip install r requirements.txt ``` #### 常见报错与解决方案 执行上述命令可能报错「Python 无法找到 TQDM 版本 >4.64.0」，解决方案： 1. 打开 `requirements.txt` 文件，将 TQDM 版本从「tqdm> 4.64.0」改为「tqdm> 4.63.0」。 2. 保存后重新执行 `pip install r requirements.txt`，即可正常安装（4.63.0 版本满足兼容性要求）。 ### 4. 检测验证环境 依赖安装完成后，执行检测命令验证环境是否正常： ```Bash python detect.py ``` 若快速输出检测结果，且与本地 Windows 环境结果一致，说明服务器环境配置成功。 ## 三、模型训练与关键问题处理 ### 1. 数据集解压与配置文件修改 ```Bash # 解压数据集（文件名按实际上传的修改） unzip dataset.zip # 复制并修改数据集配置文件 cp data/coco128.yaml data/BVN.yaml ``` 打开 `data/BVN.yaml`，修改路径与类别（适配数据集）： ```yaml path: ./dataset # 数据集根目录（相对项目路径） train: images/train val: images/val test: names: 0: mingren 1: daitu ``` 修改train.py文件,修改` data`参数的默认值，使其指向自定义的`bvn.yaml`配置文件，避免每次训练手动指定路径，代码如下： ```Python parser.add_argument(' data', type str, default ROOT / 'data/bvn.yaml', help 'dataset.yaml path') ``` 修改后，训练时程序将自动加载`data/bvn.yaml`中的数据集配置，无需额外在命令行补充` data`参数，适配自定义的`mingren`和`daitu`两类目标数据集。 ### 2. 启动训练与字体问题解决 ```Bash python train.py ``` #### 字体下载卡顿解决方案 训练时可能卡顿在「字体下载」步骤，因服务器网络不稳定，需手动上传字体文件： 1. 本地下载 Arial.ttf 字体文件，上传至服务器。 2. 在终端执行命令，将字体移动至指定目录：`mv Arial.ttf /root/.config/Ulralytics/` 3. 重新执行`python train.py`，训练正常启动，且速度显著快于本地。 ## 四、训练结果打包与本地下载 ### 1. 结果打包（解决文件夹无法直接下载问题） 训练完成后，`runs/train/` 目录下生成训练结果（如 EXP2、EXP3），因 Jupyter Lab 无法直接下载文件夹，需先打包： ```Bash # 打包训练结果 tar cvf runs.tar.gz runs/ ``` ### 2. 本地下载与使用 1. 在 Jupyter Lab 左侧找到 `runs` `.tar.gz` 文件，右键点击选择「Download」，下载至本地。 2. 本地解压后，将 EXP文件夹放入本地 YOLOv5 项目的 `runs/train/` 目录。 3. 修改检测命令中的模型路径，验证服务器训练的模型： 运行hub_detect.ipynb 运行后检测正常，说明训练结果可跨环境复用。 ## 五、服务器资源管理（成本控制） ### 1. 关机与资源释放 1. 训练完成并下载结果后，在 AutoDL 控制台找到对应实例，点击「关机」。 2. 关机后将停止计费，避免长时间占用资源导致成本浪费。 ## 六、整体流程回顾 ### 1. 核心流程 AutoDL 登录 → 算力租用（选对镜像）→ 代码/数据集上传 → 环境激活与依赖安装 → 模型训练（解决字体问题）→ 结果打包下载 → 服务器关机。"}}